bert_model:
  model_id: sentence-transformers/all-MiniLM-L12-v2
  threshold: 0.6
  use_cpu: true

semantic_cache:
  enabled: true
  backend_type: "memory"  # Options: "memory" or "milvus"
  similarity_threshold: 0.8
  max_entries: 1000  # Only applies to memory backend
  ttl_seconds: 3600
  eviction_policy: "fifo"

tools:
  enabled: true
  top_k: 3
  similarity_threshold: 0.2
  tools_db_path: "config/tools_db.json"
  fallback_to_empty: true

prompt_guard:
  enabled: true
  use_modernbert: true
  model_id: "models/jailbreak_classifier_modernbert-base_model"
  threshold: 0.7
  use_cpu: true
  jailbreak_mapping_path: "models/jailbreak_classifier_modernbert-base_model/jailbreak_type_mapping.json"

# vLLM Endpoints Configuration
# IMPORTANT: Using localhost since containers are in same pod
vllm_endpoints:
  - name: "coding-model-endpoint"
    address: "127.0.0.1"  # localhost in same pod
    port: 8000
    models:
      - "coding-model"
    weight: 1
  - name: "general-model-endpoint"
    address: "127.0.0.1"  # localhost in same pod
    port: 8001
    models:
      - "general-model"
    weight: 1

model_config:
  "coding-model":
    reasoning_family: "qwen3"  # This model uses Qwen reasoning syntax
    preferred_endpoints: ["coding-model-endpoint"]
    pii_policy:
      allow_by_default: false  # Strict PII blocking
      pii_types_allowed: ["EMAIL_ADDRESS"]  # Only allow emails
  "general-model":
    reasoning_family: "qwen3"  # This model uses Qwen reasoning syntax
    preferred_endpoints: ["general-model-endpoint"]
    pii_policy:
      allow_by_default: false  # Strict PII blocking (changed from true)
      pii_types_allowed: ["EMAIL_ADDRESS"]  # Only allow emails (same as coding-model)

# Classifier configuration
classifier:
  category_model:
    model_id: "models/category_classifier_modernbert-base_model"
    use_modernbert: true
    threshold: 0.6
    use_cpu: true
    category_mapping_path: "models/category_classifier_modernbert-base_model/category_mapping.json"
  pii_model:
    model_id: "models/pii_classifier_modernbert-base_presidio_token_model"
    use_modernbert: true
    threshold: 0.7
    use_cpu: true
    pii_mapping_path: "models/pii_classifier_modernbert-base_presidio_token_model/pii_type_mapping.json"

# Categories - Simplified to 2 categories for demo clarity
categories:
  - name: coding
    system_prompt: "You are an expert software engineer and computer scientist with deep knowledge of algorithms, data structures, programming languages (Python, Go, Java, C++, JavaScript, etc.), software architecture, design patterns, and best practices. Provide clear, practical code examples with detailed explanations. Focus on writing clean, maintainable, and efficient code. When debugging, analyze problems systematically and provide step-by-step solutions."
    model_scores:
      - model: coding-model
        score: 0.95
        use_reasoning: true  # Enable reasoning for complex coding problems
      - model: general-model
        score: 0.3
        use_reasoning: false
  - name: general
    system_prompt: "You are a knowledgeable and helpful general-purpose assistant with expertise across diverse topics including business, science, history, psychology, health, law, and everyday questions. Provide accurate, well-reasoned responses that are informative and easy to understand. When appropriate, provide context, examples, and practical advice."
    model_scores:
      - model: general-model
        score: 0.9
        use_reasoning: false
      - model: coding-model
        score: 0.2
        use_reasoning: false

default_model: coding-model

# Reasoning family configurations
reasoning_families:
  deepseek:
    type: "chat_template_kwargs"
    parameter: "thinking"

  qwen3:
    type: "chat_template_kwargs"
    parameter: "enable_thinking"

  gpt-oss:
    type: "reasoning_effort"
    parameter: "reasoning_effort"
  gpt:
    type: "reasoning_effort"
    parameter: "reasoning_effort"

# Global default reasoning effort level
default_reasoning_effort: high

# API Configuration
api:
  batch_classification:
    max_batch_size: 100
    concurrency_threshold: 5
    max_concurrency: 8
    metrics:
      enabled: true
      detailed_goroutine_tracking: true
      high_resolution_timing: false
      sample_rate: 1.0
      duration_buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30]
      size_buckets: [1, 2, 5, 10, 20, 50, 100, 200]

# Observability Configuration
observability:
  tracing:
    enabled: false  # Enable distributed tracing (default: false)
    provider: "opentelemetry"  # Provider: opentelemetry, openinference, openllmetry
    exporter:
      type: "stdout"  # Exporter: otlp, jaeger, zipkin, stdout
      endpoint: "localhost:4317"  # OTLP endpoint (when type: otlp)
      insecure: true  # Use insecure connection (no TLS)
    sampling:
      type: "always_on"  # Sampling: always_on, always_off, probabilistic
      rate: 1.0  # Sampling rate for probabilistic (0.0-1.0)
    resource:
      service_name: "vllm-semantic-router"
      service_version: "v0.1.0"
      deployment_environment: "development"