# Simulator deployment for kind clusters (no KServe required)
# Uses llm-d-inference-sim images directly
---
# Deployment 1: Model-A (llm-d-inference-sim)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-model-a
  labels:
    app: vllm-model
    model: model-a
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-model
      model: model-a
  template:
    metadata:
      labels:
        app: vllm-model
        model: model-a
    spec:
      containers:
        - name: model-a
          image: ghcr.io/llm-d/llm-d-inference-sim:v0.6.1
          imagePullPolicy: IfNotPresent
          args:
            - --port
            - "8080"
            - --model
            - Model-A
            - --mode
            - random
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 3
            periodSeconds: 5
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "200m"
---
# Deployment 2: Model-B (llm-d-inference-sim)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-model-b
  labels:
    app: vllm-model
    model: model-b
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-model
      model: model-b
  template:
    metadata:
      labels:
        app: vllm-model
        model: model-b
    spec:
      containers:
        - name: model-b
          image: ghcr.io/llm-d/llm-d-inference-sim:v0.6.1
          imagePullPolicy: IfNotPresent
          args:
            - --port
            - "8080"
            - --model
            - Model-B
            - --mode
            - random
          ports:
            - containerPort: 8080
              name: http
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 3
            periodSeconds: 5
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            requests:
              memory: "64Mi"
              cpu: "50m"
            limits:
              memory: "128Mi"
              cpu: "200m"
---
# Service for Model-A
apiVersion: v1
kind: Service
metadata:
  name: vllm-model-a
  labels:
    app: vllm-model
    model: model-a
spec:
  ports:
    - name: http
      port: 8000
      targetPort: 8080
  selector:
    app: vllm-model
    model: model-a
---
# Service for Model-B
apiVersion: v1
kind: Service
metadata:
  name: vllm-model-b
  labels:
    app: vllm-model
    model: model-b
spec:
  ports:
    - name: http
      port: 8001
      targetPort: 8080
  selector:
    app: vllm-model
    model: model-b
