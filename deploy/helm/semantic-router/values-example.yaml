# Example: Custom Configuration Values
# This file demonstrates how to customize the Semantic Router Helm deployment

# Basic settings
replicaCount: 2
nameOverride: ""
fullnameOverride: ""

# Image configuration
image:
  repository: ghcr.io/vllm-project/semantic-router/extproc
  pullPolicy: IfNotPresent
  tag: "v0.1.0"  # Use specific version for production

# Service configuration
service:
  type: ClusterIP  # Or LoadBalancer for external access
  grpc:
    port: 50051
  api:
    port: 8080
  metrics:
    enabled: true
    port: 9190

# Resources - adjust based on your workload
resources:
  limits:
    memory: "8Gi"
    cpu: "3"
  requests:
    memory: "4Gi"
    cpu: "1.5"

# Storage configuration
persistence:
  enabled: true
  storageClassName: "fast-ssd"  # Use your storage class
  size: 20Gi
  accessMode: ReadWriteOnce

# Init container configuration
initContainer:
  enabled: true
  resources:
    limits:
      memory: "2Gi"
      cpu: "1"
    requests:
      memory: "1Gi"
      cpu: "500m"

# Application configuration
config:
  # BERT model settings
  bert_model:
    model_id: sentence-transformers/all-MiniLM-L12-v2
    threshold: 0.65
    use_cpu: false  # Set to true if no GPU

  # Semantic cache
  semantic_cache:
    enabled: true
    backend_type: "memory"  # or "milvus" for production
    similarity_threshold: 0.85
    max_entries: 2000
    ttl_seconds: 7200
    eviction_policy: "lru"

  # Tools configuration
  tools:
    enabled: true
    top_k: 5
    similarity_threshold: 0.25

  # Security - Prompt guard
  prompt_guard:
    enabled: true
    use_modernbert: true
    model_id: "models/jailbreak_classifier_modernbert-base_model"
    threshold: 0.75
    use_cpu: true

  # vLLM endpoints - Configure your actual endpoints
  vllm_endpoints:
    - name: "primary-endpoint"
      address: "10.0.1.10"  # Your vLLM server IP
      port: 8000
      weight: 2
    - name: "secondary-endpoint"
      address: "10.0.1.11"  # Your vLLM server IP
      port: 8000
      weight: 1

  # Model configuration
  model_config:
    "your-model-name":
      reasoning_family: "qwen3"  # or "gpt-oss", "deepseek"
      preferred_endpoints: ["primary-endpoint"]
      pii_policy:
        allow_by_default: true

  # Default model
  default_model: "your-model-name"

  # API configuration
  api:
    batch_classification:
      max_batch_size: 150
      concurrency_threshold: 8
      max_concurrency: 12
      metrics:
        enabled: true
        sample_rate: 0.5  # Sample 50% for performance

# Auto-scaling (optional)
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 8
  targetCPUUtilizationPercentage: 75
  targetMemoryUtilizationPercentage: 80

# Ingress configuration (optional)
ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "10m"
  hosts:
    - host: semantic-router.yourdomain.com
      paths:
        - path: /
          pathType: Prefix
          servicePort: 8080
  tls:
    - secretName: semantic-router-tls
      hosts:
        - semantic-router.yourdomain.com

# Security contexts
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false
  capabilities:
    drop:
      - ALL

# Node selection (optional)
nodeSelector:
  workload-type: ml-inference

# Tolerations (optional)
tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

# Affinity rules for high availability
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - semantic-router
          topologyKey: kubernetes.io/hostname
