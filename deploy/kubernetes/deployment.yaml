apiVersion: apps/v1
kind: Deployment
metadata:
  name: semantic-router
  namespace: vllm-semantic-router-system
  labels:
    app: semantic-router
spec:
  replicas: 1
  selector:
    matchLabels:
      app: semantic-router
  template:
    metadata:
      labels:
        app: semantic-router
    spec:
      initContainers:
        - name: model-downloader
          image: python:3.11-slim
          securityContext:
            runAsNonRoot: false
            allowPrivilegeEscalation: false
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -e
              # Check if all required models already exist in PVC; if yes, skip downloads entirely
              REQUIRED_DIRS=(
                "all-MiniLM-L12-v2"
                "category_classifier_modernbert-base_model"
                "pii_classifier_modernbert-base_model"
                "jailbreak_classifier_modernbert-base_model"
                "pii_classifier_modernbert-base_presidio_token_model"
              )
              mkdir -p /app/models
              cd /app/models
              MISSING=false
              for d in "${REQUIRED_DIRS[@]}"; do
                if [ ! -d "$d" ]; then
                  MISSING=true
                  break
                fi
              done
              if [ "$MISSING" = false ]; then
                echo "All required models already present in PVC. Skipping download."
                exit 0
              fi

              echo "Installing Hugging Face CLI..."
              pip install --no-cache-dir huggingface_hub[cli]

              echo "Downloading missing models to persistent volume..."

              # Download all-MiniLM-L12-v2 model
              if [ ! -d "all-MiniLM-L12-v2" ]; then
                echo "Downloading all-MiniLM-L12-v2 model..."
                hf download sentence-transformers/all-MiniLM-L12-v2 --local-dir all-MiniLM-L12-v2
              else
                echo "all-MiniLM-L12-v2 model already exists, skipping..."
              fi

              # Download category classifier model
              if [ ! -d "category_classifier_modernbert-base_model" ]; then
                echo "Downloading category classifier model..."
                hf download LLM-Semantic-Router/category_classifier_modernbert-base_model --local-dir category_classifier_modernbert-base_model
              else
                echo "Category classifier model already exists, skipping..."
              fi

              # Download PII classifier model
              if [ ! -d "pii_classifier_modernbert-base_model" ]; then
                echo "Downloading PII classifier model..."
                hf download LLM-Semantic-Router/pii_classifier_modernbert-base_model --local-dir pii_classifier_modernbert-base_model
              else
                echo "PII classifier model already exists, skipping..."
              fi

              # Download jailbreak classifier model
              if [ ! -d "jailbreak_classifier_modernbert-base_model" ]; then
                echo "Downloading jailbreak classifier model..."
                hf download LLM-Semantic-Router/jailbreak_classifier_modernbert-base_model --local-dir jailbreak_classifier_modernbert-base_model
              else
                echo "Jailbreak classifier model already exists, skipping..."
              fi

              # Download PII token classifier model
              if [ ! -d "pii_classifier_modernbert-base_presidio_token_model" ]; then
                echo "Downloading PII token classifier model..."
                hf download LLM-Semantic-Router/pii_classifier_modernbert-base_presidio_token_model --local-dir pii_classifier_modernbert-base_presidio_token_model
              else
                echo "PII token classifier model already exists, skipping..."
              fi

              echo "All missing models downloaded successfully!"
              ls -la /app/models/
          env:
            - name: HF_HUB_CACHE
              value: /tmp/hf_cache
          # Reduced resource requirements for init container
          resources:
            requests:
              memory: "512Mi"
              cpu: "250m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          volumeMounts:
            - name: models-volume
              mountPath: /app/models
      containers:
        - name: semantic-router
          image: ghcr.io/vllm-project/semantic-router/extproc:latest
          args: ["--secure=true"]
          securityContext:
            runAsNonRoot: false
            allowPrivilegeEscalation: false
          ports:
            - containerPort: 50051
              name: grpc
              protocol: TCP
            - containerPort: 9190
              name: metrics
              protocol: TCP
            - containerPort: 8080
              name: classify-api
              protocol: TCP
          env:
            - name: LD_LIBRARY_PATH
              value: "/app/lib"
          volumeMounts:
            - name: config-volume
              mountPath: /app/config
              readOnly: true
            - name: models-volume
              mountPath: /app/models
          livenessProbe:
            tcpSocket:
              port: 50051
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            tcpSocket:
              port: 50051
            initialDelaySeconds: 90
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          # Significantly reduced resource requirements for kind cluster
          resources:
            requests:
              memory: "3Gi" # Reduced from 8Gi
              cpu: "1" # Reduced from 2
            limits:
              memory: "6Gi" # Reduced from 12Gi
              cpu: "2" # Reduced from 4
      volumes:
        - name: config-volume
          configMap:
            name: semantic-router-config
        - name: models-volume
          persistentVolumeClaim:
            claimName: semantic-router-models
