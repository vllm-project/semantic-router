# Mock LLM Backend for E2E Testing
# Uses llm-d-inference-sim to simulate vLLM-compatible API responses
# This provides instant responses for fast E2E testing without GPU requirements
apiVersion: v1
kind: Namespace
metadata:
  name: mock-llm
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mock-llm
  namespace: mock-llm
  labels:
    app: mock-llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mock-llm
  template:
    metadata:
      labels:
        app: mock-llm
    spec:
      containers:
        - name: mock-llm
          image: ghcr.io/llm-d/llm-d-inference-sim:v0.5.0
          imagePullPolicy: IfNotPresent
          args:
            - --port
            - "8000"
            - --model
            - mock-llm
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 200m
              memory: 128Mi
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 10
            periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: mock-llm
  namespace: mock-llm
  labels:
    app: mock-llm
spec:
  type: ClusterIP
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
  selector:
    app: mock-llm

