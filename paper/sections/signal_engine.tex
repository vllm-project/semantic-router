% sections/signal_engine.tex

\section{Signal Extraction Layer}
\label{sec:signal_engine}

The signal extraction layer maps an incoming request $r$ to a structured signal result that characterizes the request along eleven orthogonal dimensions.
We formalize the signal model and describe the extraction algorithms.

\subsection{Signal Model}

\begin{definition}[Signal Rule]
A \emph{signal rule} $\rho = (\tau, n, f)$ consists of a signal type $\tau \in \mathcal{T}$, a rule name $n$, and an evaluation function $f: \mathcal{R} \to \{0, 1\} \times [0, 1]$ that maps a request to a binary match indicator and a confidence score.
\end{definition}

\begin{definition}[Signal Result]
Given a rule set $\mathcal{R} = \{\rho_1, \ldots, \rho_N\}$, the signal result for request $r$ is:
\begin{equation}
  S(r) = \bigl\{ \bigl(\rho_i, \; \mathbf{1}[f_i(r)], \; c_i(r)\bigr) \mid \rho_i \in \mathcal{R} \bigr\}
\end{equation}
where $\mathbf{1}[f_i(r)]$ is the match indicator and $c_i(r) \in [0,1]$ is the confidence.
\end{definition}

The eleven signal types partition into \emph{heuristic} and \emph{learned} categories based on whether they require neural inference.

\begin{figure}[t]
\centering
\begin{tikzpicture}[
    node distance=0.4cm,
    sig/.style={rectangle, draw, rounded corners=2pt,
                minimum height=0.5cm, minimum width=1.55cm,
                align=center, font=\scriptsize, inner sep=2pt},
    arr/.style={->, >=stealth, thick},
    grp/.style={rectangle, draw, dashed, rounded corners=4pt,
                inner sep=6pt, gray!40},
  ]
  % Input request
  \node[sig, fill=black!4, minimum width=1.2cm] (req) at (0, 0) {Request $r$};

  % --- Heuristic group (top) ---
  \node[sig, fill=blue!6] (kw) at (3.0, 1.8) {$\tau_{\text{kw}}$\;Keyword};
  \node[sig, fill=blue!6] (ctx) at (5.6, 1.8) {$\tau_{\text{ctx}}$\;Context};
  \node[sig, fill=blue!6] (lang) at (3.0, 1.0) {$\tau_{\text{lang}}$\;Language};
  \node[sig, fill=blue!6] (authz) at (5.6, 1.0) {$\tau_{\text{authz}}$\;Authz};
  \node[font=\scriptsize, text=gray] (hdots) at (7.2, 1.4) {$\cdots$};
  % Heuristic bounding box (on background so it doesn't cover signal nodes)
  \begin{scope}[on background layer]
  \node[grp, fill=blue!3, fit=(kw)(ctx)(lang)(authz)(hdots),
        label={[font=\scriptsize\bfseries, text=blue!50, anchor=south]above:Heuristic {\normalfont\scriptsize($<$1\,ms)}}] (hbox) {};
  \end{scope}

  % --- Learned group (bottom) ---
  \node[sig, fill=green!6] (emb) at (2.4, -0.7) {$\tau_{\text{emb}}$\;Embed};
  \node[sig, fill=green!6] (dom) at (4.3, -0.7) {$\tau_{\text{dom}}$\;Domain};
  \node[sig, fill=green!6] (fact) at (6.2, -0.7) {$\tau_{\text{fact}}$\;Factual};
  \node[sig, fill=green!6] (fb) at (2.4, -1.5) {$\tau_{\text{fb}}$\;Feedback};
  \node[sig, fill=green!6] (mod) at (4.3, -1.5) {$\tau_{\text{mod}}$\;Modality};
  \node[sig, fill=green!6] (cpx) at (6.2, -1.5) {$\tau_{\text{cpx}}$\;Complex.};
  \node[sig, fill=green!6] (pref) at (4.3, -2.3) {$\tau_{\text{pref}}$\;Prefer.};
  \node[font=\scriptsize, text=gray] (ldots) at (6.2, -2.3) {$\cdots$};
  % Learned bounding box (on background so it doesn't cover signal nodes)
  \begin{scope}[on background layer]
  \node[grp, fill=green!3, fit=(emb)(dom)(fact)(fb)(mod)(cpx)(pref)(ldots),
        label={[font=\scriptsize\bfseries, text=green!50, anchor=north]below:Learned {\normalfont\scriptsize(10--100\,ms)}}] (lbox) {};
  \end{scope}

  % Output
  \node[sig, fill=orange!5, minimum width=1.4cm] (sr) at (8.6, 0) {$S(r)$};

  % Arrows: request → groups
  \draw[arr] (req.east) -- ++(0.4,0) |- (hbox.west);
  \draw[arr] (req.east) -- ++(0.4,0) |- (lbox.west);

  % Arrows: groups → S(r)
  \draw[arr] (hbox.east) -| ++(0.6, -0.55) -- (sr.north west);
  \draw[arr] (lbox.east) -| ++(0.6, 0.55) -- (sr.south west);

  % Parallel annotation
  \node[font=\scriptsize, text=gray, rotate=90, anchor=south] at (1.3, 0.2) {parallel};
\end{tikzpicture}
\caption{Signal extraction taxonomy and evaluation flow.  An incoming request is evaluated in parallel against heuristic signals (sub-millisecond, deterministic) and learned signals (neural inference via LoRA classifiers).  Only signal types referenced by configured decisions are computed (demand-driven evaluation).  Results merge into the structured signal result $S(r)$.}
\label{fig:signal_taxonomy}
\end{figure}

\subsection{Heuristic Signals}

Heuristic signals use deterministic or statistical algorithms with sub-millisecond latency:

\textbf{Keyword} ($\tau_\text{kw}$).
Rules are defined as pattern sets with Boolean combinators.
Each rule specifies a set of patterns $P = \{p_1, \ldots, p_k\}$ with a combinator $\in \{\textsc{and}, \textsc{or}, \textsc{nor}\}$ and one of three matching methods:
\begin{itemize}[nosep,leftmargin=1.5em]
  \item \emph{Regex} (default): compiled regular expressions with word boundaries; confidence is 1.0 on match.
  \item \emph{BM25}: BM25 scoring dispatched to a Rust-backed classifier via FFI. Each keyword is scored against the request using TF-IDF term weighting, and the rule matches when the score exceeds a configurable threshold (default 0.1). Confidence is derived from the BM25 score, providing a graded relevance signal rather than a binary match.
  \item \emph{N-gram}: character $n$-gram similarity (default trigram) dispatched to the same Rust binding. The rule matches when the Jaccard similarity between the keyword and request $n$-gram sets exceeds a threshold (default 0.4), providing inherent tolerance to typos and morphological variation without a dedicated fuzzy-matching pass.
\end{itemize}
For \textsc{and}: $f(r) = \bigwedge_{i} \text{match}(p_i, r)$; for \textsc{or}: $f(r) = \bigvee_{i} \text{match}(p_i, r)$.
The combinators apply uniformly across all three methods.

\textbf{Context Length} ($\tau_\text{ctx}$).
Rules define token-count intervals $[l, u]$.
Given estimated token count $t(r)$, the rule matches iff $l \leq t(r) \leq u$.
This enables complexity-aware routing (e.g., short queries to fast models, long contexts to extended-context models).

\textbf{Language} ($\tau_\text{lang}$).
Rules bind detected language codes to named signals using statistical n-gram detection over 100+ languages.
Enables language-specific routing (e.g., CJK queries to multilingual-specialized models).

\textbf{Authorization} ($\tau_\text{authz}$).
Role-based access control signals extracted from request headers, supporting a pluggable authentication factory.
The authz signal layer abstracts over multiple identity providers (API key, OAuth2/OIDC, cloud IAM, custom JWT, LDAP) through provider-specific extractors that resolve user identities and group memberships from credentials.
Role bindings then map resolved identities to named signals, enabling per-role routing policies (e.g., premium users routed to higher-quality models, free-tier users restricted to cost-effective models).
This \emph{inbound} authorization (who is the user and what can they access?) is complementary to the \emph{outbound} authorization factory (\Cref{subsec:authz_factory}) that injects provider-specific credentials when forwarding to backends.

\subsection{Learned Signals}

Learned signals require neural inference, typically 10--100\,ms, using the LoRA-based classifiers described in \Cref{sec:lora_mom}:

\textbf{Embedding Similarity} ($\tau_\text{emb}$).
Each rule defines reference texts $\{t_1, \ldots, t_k\}$ and a similarity threshold $\theta$.
The request embedding $\mathbf{e}_r$ is computed via a shared embedding model, and the rule matches iff:
\begin{equation}
  \max_i \cos(\mathbf{e}_r, \mathbf{e}_{t_i}) \geq \theta
\end{equation}
The confidence equals the maximum cosine similarity.
This provides scalable semantic matching without per-rule model training.

\textbf{Domain Classification} ($\tau_\text{dom}$).
A LoRA-adapted classifier trained on MMLU categories maps requests to domain labels (STEM, humanities, code, creative writing, etc.).
The classification confidence serves as the signal confidence.

\textbf{Factual Grounding} ($\tau_\text{fact}$).
A binary classifier (the HaluGate Sentinel, \Cref{sec:halugate}) determines whether the query requires factual verification, distinguishing factual questions from creative or code-generation tasks.

\textbf{User Feedback} ($\tau_\text{fb}$).
A multi-class classifier detects satisfaction, dissatisfaction, clarification requests, and preference for alternatives, enabling feedback-driven routing adjustments.

\textbf{Modality} ($\tau_\text{mod}$).
A three-class classifier (autoregressive, diffusion, both) determines the appropriate model modality for the request, trained on mixed text-generation and image-generation datasets.

\textbf{Complexity} ($\tau_\text{cpx}$).
A contrastive embedding classifier estimates query difficulty.
Each complexity rule defines two sets of candidate exemplars---\emph{hard} (e.g., multi-step reasoning problems) and \emph{easy} (e.g., simple factual lookups)---whose embeddings are precomputed at initialization.
At query time, the query embedding is compared against both sets via cosine similarity:
\begin{equation}
  \delta = \max_{h \in \mathcal{H}} \text{sim}(\mathbf{q}, \mathbf{h}) - \max_{e \in \mathcal{E}} \text{sim}(\mathbf{q}, \mathbf{e})
\end{equation}
where $\mathcal{H}$ and $\mathcal{E}$ are the hard and easy candidate sets.
The difficulty level is then $\text{hard}$ if $\delta > \theta$, $\text{easy}$ if $\delta < -\theta$, and $\text{medium}$ otherwise, for a per-rule threshold $\theta$.
Multiple complexity rules can coexist (e.g., \texttt{code\_complexity}, \texttt{math\_complexity}), each with its own candidate sets and threshold.
Composer conditions can restrict when a rule fires---for instance, evaluating \texttt{code\_complexity} only when the domain signal indicates computer science.

\textbf{Preference} ($\tau_\text{pref}$).
Personalized routing based on user interaction history.

\subsection{Parallel Evaluation with Lazy Computation}

A key optimization is \emph{demand-driven evaluation}: the engine computes only signal types referenced by at least one configured decision.
Let $\mathcal{T}_\text{used} = \bigcup_{d \in \mathcal{D}} \{\tau \mid \exists\, \text{condition in } d \text{ of type } \tau\}$.
Signal evaluators for types in $\mathcal{T}_\text{used}$ are launched as concurrent coroutines, with heuristic signals completing before learned signals due to their sub-millisecond latency.

This demand-driven approach avoids the cost of unused signal types.
In typical configurations with 3--5 active signal types out of eleven, this reduces total signal extraction latency by 50--70\% compared to exhaustive evaluation.

\subsection{Extensibility}

The eleven signal types described above represent the current built-in set; the framework is not limited to these.
The signal extraction layer defines a uniform interface---each signal type implements an evaluation function $f: \mathcal{R} \to \{0,1\} \times [0,1]$---and the decision engine references signals solely by type and rule name.
Adding a new signal type requires only implementing this interface and registering the type; no changes to the decision engine, plugin chain, or deployment infrastructure are needed.
This open architecture allows operators to introduce domain-specific signals (e.g., regulatory compliance classifiers, custom toxicity detectors) alongside the built-in types.

\subsection{Bidirectional Signal Flow}

Signals are not limited to the inbound request path.
The system also extracts signals from model \emph{responses}, enabling closed-loop routing policies that adapt based on output characteristics.
The primary example is \halugate{} (\Cref{sec:halugate}): the Sentinel classifier on the request path determines whether a query requires factual verification (the $\tau_\text{fact}$ signal), and if so, the Detector and Explainer stages analyze the model's response for unsupported claims---producing response-side detection results (confidence scores, hallucinated spans, NLI explanations) that are propagated via HTTP headers or body annotations.
This bidirectional flow---request signals gating which response analyses to perform, and response signals feeding back into observability and policy enforcement---enables adaptive quality assurance without imposing uniform overhead on all requests.

\subsection{Information-Theoretic Signal Analysis}

With $N$ signal types evaluated per request, a natural question is whether all signals contribute independently to routing quality or whether some carry redundant information.
Information theory provides the formal framework for this analysis.

For a signal type $\tau_i$ and the routing outcome variable $Y$ (the selected model), the \emph{mutual information} $I(\tau_i; Y)$ quantifies the reduction in uncertainty about the routing decision provided by observing signal $\tau_i$.
The \emph{conditional mutual information} $I(\tau_i; Y \mid \tau_j)$ measures the additional information from $\tau_i$ given that $\tau_j$ is already observed.
When $I(\tau_i; Y \mid \tau_j) \approx 0$, signals $\tau_i$ and $\tau_j$ are redundant with respect to routing---observing both provides no more discriminative power than observing one.

This analysis enables two optimizations.
First, \emph{adaptive signal pruning}: in a given deployment configuration, signals with near-zero mutual information with the routing outcome can be disabled without affecting routing quality, reducing extraction latency beyond the demand-driven approach of \Cref{sec:signal_engine}.
Second, \emph{information-ordered evaluation}: evaluating high-$I(\tau_i; Y)$ signals first and short-circuiting when the decision outcome is already determined---analogous to early termination in decision trees---can reduce average per-request evaluation cost.
The minimum description length (MDL) principle~\cite{rissanen1978modeling} provides a complementary perspective: the optimal signal subset is the one that describes the routing policy with minimum total code length, balancing signal extraction cost against routing precision.
