% sections/memory_rag.tex

\section{Memory and Retrieval-Augmented Generation}
\label{sec:memory_rag}

Production routing systems must support multi-turn conversations with persistent context and knowledge-augmented responses.
We describe the memory and RAG subsystems that operate as plugins within the routing pipeline.

\subsection{Persistent Memory}

The memory system maintains user-scoped knowledge across conversation sessions, enabling personalized routing and context-aware responses.
\Cref{fig:memory_lifecycle} illustrates the full memory lifecycle.

\textbf{Memory extraction.}
An LLM-based extractor analyzes conversation turns to identify user-specific facts, classified into three types:
\emph{semantic} (factual knowledge: preferences, background),
\emph{procedural} (workflows, how-to knowledge), and
\emph{episodic} (specific events and interactions).

\textbf{Deduplication.}
Before storage, extracted facts undergo similarity-based deduplication against existing memories, preventing redundant entries that would degrade retrieval precision.

\textbf{Retrieval gating.}
Not every query benefits from memory retrieval.
A lightweight heuristic determines whether memory search is warranted by filtering out general fact-check queries, tool-augmented requests, and simple greetings, avoiding unnecessary embedding lookups and reducing latency for queries where personal context is irrelevant.

\textbf{Retrieval.}
At query time, relevant memories are retrieved via embedding similarity search over the user's memory store, formatted as context, and injected into the system message.
An optional query-rewriting step reformulates the user's query for improved retrieval recall.

\textbf{Retention scoring and pruning.}
Memory stores grow unbounded without lifecycle management.
We adopt an exponential decay model inspired by the Ebbinghaus forgetting curve~\cite{zhong2023memorybank}:
\begin{equation}
  R = e^{-t/S}, \quad S = S_0 + n_{\text{access}}
\end{equation}
where $t$ is the time in days since last retrieval, $S_0$ is the initial strength (default 30 days), and $n_{\text{access}}$ is the cumulative access count.
Each retrieval reinforces the memory by incrementing $n_{\text{access}}$, slowing future decay.
Memories falling below a retention threshold $R < \delta$ become pruning candidates.
An optional per-user capacity limit evicts the lowest-scoring entries when the store exceeds a configurable maximum.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.6cm and 0.8cm,
    box/.style={draw, rounded corners, fill=blue!8, minimum width=2.2cm,
                minimum height=0.8cm, align=center, font=\small},
    storebox/.style={draw, rounded corners, fill=green!10, minimum width=2.2cm,
                minimum height=0.8cm, align=center, font=\small},
    typebox/.style={draw, rounded corners, fill=orange!10,
                minimum height=0.55cm, align=center, font=\scriptsize},
    arr/.style={->, >=stealth, thick},
    darr/.style={->, >=stealth, thick, dashed},
    lbl/.style={font=\scriptsize, text=gray}
]

% --- Write path (top row) ---
\node[box] (conv) {Conversation\\Turns};
\node[box, right=of conv] (extract) {LLM\\Extraction};
\node[box, right=of extract] (dedup) {Dedup\\Check};
\node[storebox, right=of dedup] (store) {Memory\\Store};

\draw[arr] (conv) -- (extract);
\draw[arr] (extract) -- (dedup);
\draw[arr] (dedup) -- node[above, lbl] {new} (store);

% Memory types below extraction
\node[typebox, below=0.4cm of extract, xshift=-1.2cm] (sem) {semantic};
\node[typebox, right=0.2cm of sem] (proc) {procedural};
\node[typebox, right=0.2cm of proc] (epis) {episodic};
\draw[darr] (extract) -- (proc);

% Dedup "update existing" loops back above store
\draw[darr] (dedup.north) -- ++(0,0.45) -| node[above, lbl, pos=0.25] {update existing} (store.north);

% --- Read path (bottom row) ---
\node[box, below=2.8cm of conv] (query) {User\\Query};
\node[box, right=of query] (gate) {Retrieval\\Gating};
\node[box, right=of gate] (retrieve) {Similarity\\Search};
\node[box, right=of retrieve] (inject) {Inject into\\System Msg};

\draw[arr] (query) -- (gate);
\draw[arr] (gate) -- node[above, lbl] {pass} (retrieve);
\draw[arr] (retrieve) -- (inject);

% Store feeds into retrieval
\draw[arr] (store.south) -- ++(0,-1.15) -| (retrieve.north);

% Skip arrow from gate
\draw[darr] (gate.south) -- ++(0,-0.5) node[below, lbl] {skip};

% Retrieval reinforces memory (feedback arrow on the right)
\draw[darr] (inject.north) -- ++(0,0.6) node[right, lbl] {$n_{\text{access}}$\,+\,1} |- (store.east);

% --- Path labels ---
\node[lbl, above=0.08cm of conv, xshift=1.2cm] {\textit{Write path}};
\node[lbl, above=0.08cm of query, xshift=1.2cm] {\textit{Read path}};

\end{tikzpicture}
\caption{Memory lifecycle. The write path extracts facts from conversations, deduplicates against existing entries, and stores new memories. The read path gates retrieval, searches by embedding similarity, and injects context. Each retrieval increments $n_{\text{access}}$, reinforcing the memory's retention score.}
\label{fig:memory_lifecycle}
\end{figure}

\subsection{Retrieval-Augmented Generation}

The RAG plugin retrieves relevant documents from vector stores and injects them as context before model invocation.

\textbf{Indexing pipeline.}
Documents are chunked (configurable size and overlap), embedded using the shared embedding model (\Cref{sec:ml_inference}), and indexed in a vector store.

\textbf{Retrieval.}
For each request, the query embedding is computed and the top-$k$ most similar chunks are retrieved via approximate nearest neighbor search.
Retrieved chunks are formatted and injected into the prompt as additional context.

\textbf{Backend abstraction.}
Vector stores are accessed through a common interface supporting in-memory (development), Milvus~\cite{wang2021milvus} (production), and file-based (persistent, no external dependencies) backends.

\subsection{Stateful Conversations (Response API)}

The system supports the OpenAI Responses API for stateful multi-turn conversations:

\textbf{Conversation chaining.}
Each response is stored with a unique ID.
Subsequent requests reference \texttt{previous\_response\_id} to reconstruct the full conversation history.
A bidirectional translator converts between the Response API format and Chat Completions format for backend model invocation.

\textbf{Routing continuity.}
Stored responses include routing metadata (decision, model selection, signal results), enabling consistent routing across conversation turns and providing context for feedback-driven model selection.

\textbf{State backends.}
Conversation state is persisted via in-memory (development), Redis (distributed), or Milvus (semantic retrieval) backends.

\subsection{Integration with Signal-Decision Architecture}

Both memory and RAG operate as per-decision plugins.
Different decisions can activate different RAG configurations (different vector stores, different $k$ values, different chunk strategies) or disable retrieval entirely.
This enables, for example, a ``research assistant'' decision that activates RAG with a technical knowledge base while a ``casual chat'' decision disables retrieval.
