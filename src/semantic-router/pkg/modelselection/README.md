# ML-Based Model Selection for Semantic Router

> **Issue**: [#986 - Implement ML-based model selection techniques](https://github.com/vllm-project/semantic-router/issues/986)  
> **Milestone**: v0.2 - Athena  
> **Reference**: [vLLM SR Iris Blog](https://blog.vllm.ai/2026/01/05/vllm-sr-iris.html)
>
> **ðŸ“ Note on Training Data:**  
> All training data files (`training_data_with_category.jsonl`, `benchmark_training_data.jsonl`, `trained_models/*.json`) are **deployment-specific** and should be generated by each user based on their own LLMs, queries, and benchmarks. These files are NOT included in this repository. See [Training Data Format](#training-data-format) for required file formats.

## Overview

This module implements machine learning-based model selection algorithms that intelligently route queries to the most appropriate LLM based on query characteristics, historical performance, and learned patterns.

### Implemented Algorithms

| Algorithm | Implementation | Linfa Crate | Best For | Key Parameters |
|-----------|----------------|-------------|----------|----------------|
| **KNN** (K-Nearest Neighbors) | Rust (`ml-binding/`) | `linfa-nn` | Finding similar queries using Ball Tree | `k` (neighbors) |
| **KMeans** | Rust (`ml-binding/`) | `linfa-clustering` | Cluster-based routing (Avengers-Pro) | `num_clusters` |
| **SVM** (Support Vector Machine) | Rust (`ml-binding/`) | `linfa-svm` | Decision boundaries with RBF kernel | `kernel` |
| **MLP** (Multi-Layer Perceptron) | Go (gonum) | N/A | Complex query-model patterns | `hidden_layers` |
| **Matrix Factorization** | Go (gonum) | N/A | Collaborative filtering (RouteLLM) | `num_factors` |

> **Note:** KNN, KMeans, and SVM use [Linfa](https://github.com/rust-ml/linfa) via `ml-binding/`.
> MLP and Matrix Factorization use pure Go implementations (Linfa doesn't support these algorithms).
>
> **Reference:** Implementation aligned with [FusionFactory (arXiv:2507.10540)](https://arxiv.org/abs/2507.10540) query-level fusion approach.

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         OFFLINE TRAINING                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ MMLU/Custom  â”‚â”€â”€â”€â–¶â”‚  Benchmark   â”‚â”€â”€â”€â–¶â”‚ benchmark_training   â”‚  â”‚
â”‚  â”‚   Queries    â”‚    â”‚   Runner     â”‚    â”‚    _data.jsonl       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚                       â”‚               â”‚
â”‚                             â–¼                       â–¼               â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                      â”‚   Ollama/   â”‚        â”‚  Embedding  â”‚        â”‚
â”‚                      â”‚   vLLM      â”‚        â”‚   Model     â”‚        â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  (Qwen3)    â”‚        â”‚
â”‚                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                    â”‚                â”‚
â”‚                                                    â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    TRAINER                                   â”‚   â”‚
â”‚  â”‚  â€¢ Generates embeddings (768-dim)                           â”‚   â”‚
â”‚  â”‚  â€¢ Creates training records with category features          â”‚   â”‚
â”‚  â”‚  â€¢ Trains 5 algorithms                                      â”‚   â”‚
â”‚  â”‚  â€¢ Saves model parameters to JSON                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                               â”‚                                     â”‚
â”‚                               â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              TRAINED MODEL FILES                             â”‚   â”‚
â”‚  â”‚  â€¢ knn_model.json      (embeddings + best models)           â”‚   â”‚
â”‚  â”‚  â€¢ kmeans_model.json   (cluster centroids)                  â”‚   â”‚
â”‚  â”‚  â€¢ mlp_model.json      (weights + biases)                   â”‚   â”‚
â”‚  â”‚  â€¢ svm_model.json      (support vectors + alphas)           â”‚   â”‚
â”‚  â”‚  â€¢ mf_model.json       (model factors + projection)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ONLINE INFERENCE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  User Query  â”‚â”€â”€â”€â–¶â”‚  Embedding   â”‚â”€â”€â”€â–¶â”‚  Feature Vector      â”‚  â”‚
â”‚  â”‚              â”‚    â”‚   Model      â”‚    â”‚  (768 + 14 = 782)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                    â”‚                â”‚
â”‚                                                    â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    SELECTOR                                  â”‚   â”‚
â”‚  â”‚  â€¢ Loads trained model from JSON                            â”‚   â”‚
â”‚  â”‚  â€¢ Applies algorithm-specific inference                     â”‚   â”‚
â”‚  â”‚  â€¢ Returns best model for query                             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                               â”‚                                     â”‚
â”‚                               â–¼                                     â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚                      â”‚  Selected   â”‚                               â”‚
â”‚                      â”‚    LLM      â”‚                               â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## File Structure

```
src/semantic-router/pkg/modelselection/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ selector.go                  # Core algorithm implementations
â”œâ”€â”€ trainer.go                   # Training logic and record generation
â”œâ”€â”€ persistence.go               # JSON serialization/deserialization
â”œâ”€â”€ selector_load_test.go        # Unit and integration tests
â””â”€â”€ data/
    â”œâ”€â”€ training_data_with_category.jsonl  # Source queries with categories
    â”œâ”€â”€ benchmark_training_data.jsonl                 # Generated benchmark data
    â””â”€â”€ trained_models/
        â”œâ”€â”€ knn_model.json       # KNN trained model
        â”œâ”€â”€ kmeans_model.json    # KMeans trained model
        â”œâ”€â”€ mlp_model.json       # MLP trained model
        â”œâ”€â”€ svm_model.json       # SVM trained model
        â””â”€â”€ mf_model.json        # Matrix Factorization trained model
```

---

## Algorithm Details

### 1. KNN (K-Nearest Neighbors)

**How it works:**

- Stores all training embeddings with their associated best-performing model
- For new queries, finds K nearest neighbors using cosine similarity
- Returns the model with most votes among neighbors

**JSON Structure (`knn_model.json`):**

```json
{
  "algorithm": "knn",
  "trained": true,
  "k": 5,
  "training_records": [
    {
      "embedding": [0.123, -0.456, ...],  // 782-dim vector
      "best_model": "llama-3.2-3b",
      "category": "math"
    }
  ]
}
```

### 2. KMeans (Clustering)

**How it works:**

- Clusters training embeddings into K groups
- Assigns the best-performing model to each cluster centroid
- For new queries, finds nearest centroid and returns its model

**JSON Structure (`kmeans_model.json`):**

```json
{
  "algorithm": "kmeans",
  "trained": true,
  "num_clusters": 8,
  "centroids": [
    [0.123, -0.456, ...],  // 782-dim centroid vectors
    ...
  ],
  "cluster_models": ["llama-3.2-1b", "llama-3.2-3b", ...]
}
```

### 3. MLP (Multi-Layer Perceptron)

**How it works:**

- Neural network with configurable hidden layers
- Input: 782-dim feature vector (768 embedding + 14 category one-hot)
- Output: Score per model
- Uses ReLU activation and softmax output

**JSON Structure (`mlp_model.json`):**

```json
{
  "algorithm": "mlp",
  "trained": true,
  "input_dim": 782,
  "hidden_layers": [64, 32],
  "weights": [
    [[...], [...], ...],  // Layer 1: 782 x 64
    [[...], [...], ...],  // Layer 2: 64 x 32
    [[...], [...], ...]   // Output: 32 x num_models
  ],
  "biases": [
    [...],  // Layer 1 biases
    [...],  // Layer 2 biases
    [...]   // Output biases
  ]
}
```

### 4. SVM (Support Vector Machine)

**How it works:**

- Binary classifiers for each model (one-vs-all)
- Uses RBF kernel by default
- Stores support vectors and alpha coefficients
- Model with highest decision score wins

**JSON Structure (`svm_model.json`):**

```json
{
  "algorithm": "svm",
  "trained": true,
  "kernel": "rbf",
  "support_vectors": {
    "0": [[...], [...], ...],  // Support vectors for model 0
    "1": [[...], [...], ...]   // Support vectors for model 1
  },
  "alphas": {
    "0": [...],  // Alpha coefficients for model 0
    "1": [...]   // Alpha coefficients for model 1
  },
  "biases": [0.5, -0.3, ...]
}
```

### 5. Matrix Factorization

**How it works:**

- Collaborative filtering approach for model selection
- Projects query embeddings into latent factor space
- Learns model factor vectors
- Score = dot(projection(embedding), model_factors) + bias

**JSON Structure (`mf_model.json`):**

```json
{
  "algorithm": "matrix_factorization",
  "trained": true,
  "num_factors": 10,
  "input_dim": 782,
  "model_factors": {
    "llama-3.2-1b": [...],  // 10-dim factor vector
    "llama-3.2-3b": [...]
  },
  "projection": [[...], [...], ...],  // 782 x 10 matrix
  "model_biases": {
    "llama-3.2-1b": 0.5,
    "llama-3.2-3b": 0.3
  },
  "global_bias": 0.1
}
```

---

## Embedding & Feature Vector Generation

### How Query Embedding Works

When a query arrives, it goes through a **two-stage embedding process**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query: "What is the derivative of sin(x)?"                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 1: Text Embedding (Candle + Qwen3)                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  â€¢ Uses Qwen3-Embedding model via Candle (Rust ML framework)       â”‚
â”‚  â€¢ Converts query text to dense semantic vector                    â”‚
â”‚  â€¢ Output: 768-dimensional float vector                            â”‚
â”‚                                                                     â”‚
â”‚  embedding = [0.0234, -0.1456, 0.0821, ..., 0.0512]  â† 768 dims    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 2: Category One-Hot Encoding                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚  â€¢ Category determined by domain classifier                        â”‚
â”‚  â€¢ Converted to binary vector (1 for matching category, 0 else)    â”‚
â”‚  â€¢ Output: 14-dimensional binary vector                            â”‚
â”‚                                                                     â”‚
â”‚  category = "math"                                                  â”‚
â”‚  one_hot  = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  â† 14 dims  â”‚
â”‚              â†‘                                                      â”‚
â”‚            math                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STAGE 3: Concatenation â†’ Feature Vector                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚                                                                     â”‚
â”‚  feature_vector = embedding âŠ• category_one_hot                     â”‚
â”‚                                                                     â”‚
â”‚  [0.0234, -0.1456, ..., 0.0512, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 768 dims â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 14 dims â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                                                                     â”‚
â”‚  Total: 782 dimensions                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Combine Query + Category?

| Component | Purpose |
|-----------|---------|
| **Query Embedding (768d)** | Captures semantic meaning - "what the query is about" |
| **Category One-Hot (14d)** | Explicit domain signal - helps algorithms learn domain-specific patterns |

**Example:** Two queries might have similar embeddings:

- "Calculate the integral of xÂ²" â†’ math
- "Calculate the trajectory of a projectile" â†’ physics

The category one-hot helps the model distinguish that **different LLMs** may excel in different domains, even for semantically similar queries.

### Embedding Model

We use **Candle** (Rust ML framework) with **Qwen3-Embedding** model (same as rest of VSR):

```go
// trainer.go - uses candle_binding directly
import candle_binding "github.com/vllm-project/semantic-router/candle-binding"

// Qwen3 (default) - high quality, supports batching
output, err := candle_binding.GetEmbeddingBatched(text, "qwen3", 768)

// Gemma - faster inference
output, err := candle_binding.GetEmbeddingWithModelType(text, "gemma", 768)

// BERT - legacy support
embedding, err := candle_binding.GetEmbedding(text, 768)
```

**Supported Models:**

| Model | Path | Dim | Use Case |
|-------|------|-----|----------|
| **Qwen3** (default) | `models/mom-embedding-pro` | 768 | High quality, long context |
| **Gemma** | `models/embeddinggemma-300m` | 768 | Low latency |
| **BERT** | Built-in | 768 | Legacy/fallback |

**Fallback Mode:** Use `--use-fallback` flag for hash-based embeddings (no Candle required)

### Category Mapping

```go
// trainer.go
var categoryIndex = map[string]int{
    "math":             0,
    "physics":          1,
    "chemistry":        2,
    "biology":          3,
    "computer_science": 4,
    "history":          5,
    "economics":        6,
    "health":           7,
    "psychology":       8,
    "law":              9,
    "business":         10,
    "philosophy":       11,
    "engineering":      12,
    "other":            13,
}

func categoryToOneHot(category string) []float64 {
    oneHot := make([]float64, 14)
    if idx, ok := categoryIndex[category]; ok {
        oneHot[idx] = 1.0
    } else {
        oneHot[13] = 1.0  // Default to "other"
    }
    return oneHot
}
```

### Complete Feature Vector Structure

| Index | Content | Description |
|-------|---------|-------------|
| 0-767 | `embedding[0:768]` | Qwen3 semantic embedding |
| 768 | `category == "math"` | 1.0 if math, else 0.0 |
| 769 | `category == "physics"` | 1.0 if physics, else 0.0 |
| 770 | `category == "chemistry"` | 1.0 if chemistry, else 0.0 |
| 771 | `category == "biology"` | 1.0 if biology, else 0.0 |
| 772 | `category == "computer_science"` | 1.0 if CS, else 0.0 |
| 773 | `category == "history"` | 1.0 if history, else 0.0 |
| 774 | `category == "economics"` | 1.0 if economics, else 0.0 |
| 775 | `category == "health"` | 1.0 if health, else 0.0 |
| 776 | `category == "psychology"` | 1.0 if psychology, else 0.0 |
| 777 | `category == "law"` | 1.0 if law, else 0.0 |
| 778 | `category == "business"` | 1.0 if business, else 0.0 |
| 779 | `category == "philosophy"` | 1.0 if philosophy, else 0.0 |
| 780 | `category == "engineering"` | 1.0 if engineering, else 0.0 |
| 781 | `category == "other"` | 1.0 if other/unknown, else 0.0 |

---

## Training Data Format

> **âš ï¸ IMPORTANT: Data Files Are Deployment-Specific**
>
> Training data files are **NOT included in this repository**. Each deployment should generate its own training data based on:
>
> - Your specific LLM models (Ollama, OpenAI, vLLM, etc.)
> - Your query patterns and use cases
> - Your performance benchmarks
>
> The files referenced below (`training_data_with_category.jsonl`, `benchmark_training_data.jsonl`, `trained_models/*.json`) are generated by YOU during the training process.

### What You Provide vs. What System Generates

| Field | Source | File |
|-------|--------|------|
| `query` | âœï¸ **You provide** | Your input file |
| `category` | âœï¸ **You provide** (or use script) | Your input file |
| `ground_truth` | âœï¸ **You provide** (recommended) | Your input file |
| `model_name` | ðŸ¤– **System generates** | `benchmark_training_data.jsonl` |
| `response` | ðŸ¤– **System generates** (LLM output) | `benchmark_training_data.jsonl` |
| `response_time` | ðŸ¤– **System generates** (latency) | `benchmark_training_data.jsonl` |
| `performance` | ðŸ¤– **System generates** (quality) | `benchmark_training_data.jsonl` |

**Data Flow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOUR INPUT FILE                    â”‚
â”‚  (training_data_with_category.jsonl)â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  query: "What is 2+2?"              â”‚
â”‚  category: "math"                   â”‚
â”‚  ground_truth: "4"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼  vsr train --benchmark
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BENCHMARK RUNNER                   â”‚
â”‚  â€¢ Sends query to each LLM          â”‚
â”‚  â€¢ Measures response time (latency) â”‚
â”‚  â€¢ Captures model response          â”‚
â”‚  â€¢ Calculates quality score         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GENERATED: benchmark_training_data â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  query: "What is 2+2?"              â”‚  â† from your input
â”‚  category: "math"                   â”‚  â† from your input
â”‚  ground_truth: "4"                  â”‚  â† from your input
â”‚  model_name: "llama-3.2-1b"         â”‚  â† GENERATED
â”‚  response: "The answer is 4"        â”‚  â† GENERATED (LLM output)
â”‚  response_time: 0.523               â”‚  â† GENERATED (latency)
â”‚  performance: 1.0                   â”‚  â† GENERATED (quality)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 1: Prepare Your Training Data

Your training data file should be a JSONL file with one JSON object per line. This is YOUR dataset - name it anything (e.g., `my_training_data.jsonl`, `queries.jsonl`).

**Required Fields:**

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `query` | string | âœ… Yes | The input query/prompt text |
| `category` | string | âœ… Yes | Query category (see categories below) |
| `ground_truth` | string | âš¡ Recommended | Expected correct answer (for quality scoring) |
| `task_name` | string | Optional | Task identifier (e.g., "mmlu", "gsm8k") |

**Supported Categories:**

```
math, physics, chemistry, biology, computer_science, engineering,
history, geography, literature, economics, law, medicine, 
philosophy, other
```

**Minimal Format (query + category only):**

```jsonl
{"query": "What is the integral of x^2?", "category": "math"}
{"query": "Explain quantum entanglement", "category": "physics"}
{"query": "Write a Python function to sort a list", "category": "computer_science"}
```

**Recommended Format (with ground_truth for quality scoring):**

```jsonl
{"query": "What is 2 + 2?", "category": "math", "ground_truth": "4"}
{"query": "What is the capital of France?", "category": "geography", "ground_truth": "Paris"}
{"query": "Write a function to reverse a string in Python", "category": "computer_science", "ground_truth": "def reverse(s): return s[::-1]"}
```

**Full Format (MMLU-style with multiple choice):**

```jsonl
{
  "task_name": "mmlu",
  "query": "What is the derivative of x^3?\nA) x^2\nB) 3x^2\nC) 3x\nD) x^3",
  "category": "math",
  "ground_truth": "B"
}
```

### Step 2: Add Categories (if needed)

If your data doesn't have categories, use the provided script:

```bash
cd src/semantic-router/pkg/modelselection/data
python add_category_to_training_data.py \
  --input your_training_data.jsonl \
  --output training_data_with_category.jsonl
```

### Step 3: Generated Benchmark Data

After running `vsr train --benchmark`, the system generates `benchmark_training_data.jsonl` with performance data for each model:

```jsonl
{
  "task_name": "mmlu",
  "query": "What is the integral of x^2?",
  "category": "math",
  "model_name": "llama-3.2-1b",
  "response": "The integral of x^2 is x^3/3 + C",
  "ground_truth": "x^3/3 + C",
  "response_time": 1.234,
  "performance": 0.85,
  "embedding_id": 1
}
{
  "task_name": "mmlu",
  "query": "What is the integral of x^2?",
  "category": "math",
  "model_name": "llama-3.2-3b",
  "response": "To find the integral of x^2, we use the power rule...",
  "ground_truth": "x^3/3 + C",
  "response_time": 2.567,
  "performance": 0.92,
  "embedding_id": 1
}
```

**Benchmark Data Fields:**

| Field | Type | Description |
|-------|------|-------------|
| `query` | string | Original query text |
| `category` | string | Query category |
| `model_name` | string | LLM that processed this query |
| `response` | string | Model's response |
| `ground_truth` | string | Expected answer (if provided) |
| `response_time` | float | Latency in seconds |
| `performance` | float | Quality score 0-1 |
| `embedding_id` | int | Groups responses for same query |

### Quality Scoring

The system calculates quality scores based on `ground_truth`:

| Query Type | Scoring Method |
|------------|----------------|
| Multiple Choice (A/B/C/D) | Exact match of selected option |
| Numeric/Math | Parse and compare numbers |
| Text/Code | F1 score between response and ground_truth |
| No ground_truth | Uses `performance` field if available, else 0.5 |

---

## Configuration

### Supported LLM Providers

> **Note:** This documentation uses **Ollama** as an example for local development and testing. 
> However, the model selection system supports **any LLM provider** with proper authentication.

| Provider | Type | Authentication | Use Case |
|----------|------|----------------|----------|
| **Ollama** | `ollama` | None (local) | Local development, testing |
| **vLLM** | `vllm` | None or API key | Self-hosted production |
| **OpenAI** | `openai` | API key | GPT-4, GPT-3.5 |
| **HuggingFace** | `huggingface` | HF Token | Inference API |
| **NVIDIA NIM** | `nvidia` | API key | Enterprise deployment |
| **OpenRouter** | `openrouter` | API key | Multi-provider gateway |

### Example 1: Ollama (Local - No Auth)

```yaml
# Used in this example for local testing
vllm_endpoints:
  - name: "ollama"
    address: "localhost"
    port: 11434
    type: "ollama"
    weight: 1

model_config:
  "llama-3.2-1b":
    preferred_endpoints: ["ollama"]
    external_model_ids:
      ollama: "llama3.2:1b"

  "llama-3.2-3b":
    preferred_endpoints: ["ollama"]
    external_model_ids:
      ollama: "llama3.2:3b"
```

### Example 2: HuggingFace (Token Auth)

```yaml
vllm_endpoints:
  - name: "huggingface"
    address: "api-inference.huggingface.co"
    port: 443
    type: "huggingface"
    api_key: "${HF_TOKEN}"  # Environment variable
    weight: 1

model_config:
  "mistral-7b-hf":
    preferred_endpoints: ["huggingface"]
    external_model_ids:
      huggingface: "mistralai/Mistral-7B-Instruct-v0.2"
    access_key: "${HF_TOKEN}"  # Model-specific token if needed
```

### Example 3: OpenAI (API Key)

```yaml
vllm_endpoints:
  - name: "openai"
    address: "api.openai.com"
    port: 443
    type: "openai"
    api_key: "${OPENAI_API_KEY}"
    weight: 1

model_config:
  "gpt-4":
    preferred_endpoints: ["openai"]
    external_model_ids:
      openai: "gpt-4-turbo"

  "gpt-3.5":
    preferred_endpoints: ["openai"]
    external_model_ids:
      openai: "gpt-3.5-turbo"
```

### Example 4: NVIDIA NIM (Enterprise)

```yaml
vllm_endpoints:
  - name: "nvidia-nim"
    address: "integrate.api.nvidia.com"
    port: 443
    type: "nvidia"
    api_key: "${NVIDIA_API_KEY}"
    weight: 1

model_config:
  "llama-3.1-70b-nim":
    preferred_endpoints: ["nvidia-nim"]
    external_model_ids:
      nvidia: "meta/llama-3.1-70b-instruct"
```

### Example 5: Mixed Providers (Production)

You can mix multiple providers in a single configuration:

```yaml
vllm_endpoints:
  # Local Ollama for fast/cheap models
  - name: "ollama-local"
    address: "localhost"
    port: 11434
    type: "ollama"
    weight: 1

  # OpenAI for premium models
  - name: "openai"
    address: "api.openai.com"
    port: 443
    type: "openai"
    api_key: "${OPENAI_API_KEY}"
    weight: 1

  # HuggingFace for open models
  - name: "huggingface"
    address: "api-inference.huggingface.co"
    port: 443
    type: "huggingface"
    api_key: "${HF_TOKEN}"
    weight: 1

model_config:
  # Fast local model for simple queries
  "llama-3.2-1b":
    preferred_endpoints: ["ollama-local"]
    external_model_ids:
      ollama: "llama3.2:1b"

  # Premium model for complex reasoning
  "gpt-4":
    preferred_endpoints: ["openai"]
    external_model_ids:
      openai: "gpt-4-turbo"

  # Open model for code generation
  "codellama-34b":
    preferred_endpoints: ["huggingface"]
    external_model_ids:
      huggingface: "codellama/CodeLlama-34b-Instruct-hf"
    access_key: "${HF_TOKEN}"
```

### Authentication Options

| Method | Config Field | Example |
|--------|--------------|---------|
| **Endpoint-level API key** | `api_key` in endpoint | `api_key: "${OPENAI_API_KEY}"` |
| **Model-level access key** | `access_key` in model_config | `access_key: "${HF_TOKEN}"` |
| **Environment variable** | `${VAR_NAME}` syntax | Auto-resolved at runtime |
| **Direct value** | Plain string | `api_key: "sk-..."` (not recommended) |

### Full Example: 4 Models with Ollama (This Documentation)

```yaml
# Model Selection Configuration (global settings)
model_selection:
  enabled: true
  models_path: "src/semantic-router/pkg/modelselection/data/trained_models"
  embedding_dim: 768

# vLLM/Ollama Endpoints
vllm_endpoints:
  - name: "ollama"
    address: "localhost"
    port: 11434
    type: "ollama"
    weight: 1

# Model configurations - define each LLM
model_config:
  "llama-3.2-1b":
    preferred_endpoints: ["ollama"]
    external_model_ids:
      ollama: "llama3.2:1b"

  "llama-3.2-3b":
    preferred_endpoints: ["ollama"]
    external_model_ids:
      ollama: "llama3.2:3b"

  "codellama-7b":
    preferred_endpoints: ["ollama"]
    external_model_ids:
      ollama: "codellama:7b"

  "mistral-7b":
    preferred_endpoints: ["ollama"]
    external_model_ids:
      ollama: "mistral:7b"
```

### Per-Decision Algorithm Configuration

```yaml
decisions:
  - name: "math_decision"
    description: "Mathematics and quantitative reasoning"
    priority: 100
    rules:
      operator: "AND"
      conditions:
        - type: "domain"
          name: "math"
    algorithm:
      type: "knn"          # Choose: knn, kmeans, mlp, svm, matrix_factorization
      knn:
        k: 5               # Algorithm-specific parameters
    modelRefs:
      - model: "llama-3.2-1b"
        use_reasoning: false
      - model: "llama-3.2-3b"
        use_reasoning: false
      - model: "codellama-7b"
        use_reasoning: false
      - model: "mistral-7b"
        use_reasoning: false
```

---

## Training Commands

### Prerequisites

> âš ï¸ **IMPORTANT:** Real Qwen3 embeddings are required for both training and production.
> The hash-based fallback should only be used for quick development testing.

#### 1. Start Ollama and pull models

```bash
ollama serve &
ollama pull llama3.2:1b
ollama pull llama3.2:3b
ollama pull codellama:7b
ollama pull mistral:7b
```

#### 2. Build Candle Library (Required for Real Embeddings)

The Candle library provides Qwen3 embeddings. You MUST build it before training.

**For WSL (Windows Subsystem for Linux):**

```bash
# Navigate to candle-binding directory
cd /mnt/c/vllmproject/semantic-router/candle-binding

# Build the Rust library (CPU-only, no CUDA required)
cargo build --release --no-default-features

# Set library path (required for each terminal session)
export LD_LIBRARY_PATH=$(pwd)/target/release:$LD_LIBRARY_PATH

# Verify the library exists
ls -la target/release/libcandle_semantic_router.so
```

**For Native Linux (CPU-only):**

```bash
# Navigate to candle-binding directory
cd /path/to/semantic-router/candle-binding

# Build the Rust library (CPU-only, no CUDA required)
cargo build --release --no-default-features

# Set library path (add to ~/.bashrc for persistence)
export LD_LIBRARY_PATH=$(pwd)/target/release:$LD_LIBRARY_PATH
echo 'export LD_LIBRARY_PATH=/path/to/semantic-router/candle-binding/target/release:$LD_LIBRARY_PATH' >> ~/.bashrc

# Verify the library exists
ls -la target/release/libcandle_semantic_router.so
```

**For Native Linux (with NVIDIA GPU):**

```bash
# Requires CUDA toolkit installed (nvcc in PATH)
cd /path/to/semantic-router/candle-binding

# Build with GPU acceleration
cargo build --release

# Set library path
export LD_LIBRARY_PATH=$(pwd)/target/release:$LD_LIBRARY_PATH
```

**For Docker/CI:**

```dockerfile
# In your Dockerfile (CPU-only)
WORKDIR /app/candle-binding
RUN cargo build --release --no-default-features
ENV LD_LIBRARY_PATH=/app/candle-binding/target/release:$LD_LIBRARY_PATH
```

**Troubleshooting Candle Build:**

- Ensure Rust is installed: `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`
- If build fails, check for missing dependencies: `sudo apt-get install build-essential pkg-config libssl-dev`
- Verify `LD_LIBRARY_PATH` is set correctly before running training

### Step 1: Add Category Field to Training Data

If your training data doesn't have category labels:

```bash
cd src/semantic-router/pkg/modelselection/data
python add_category_to_training_data.py \
  --input training_data.jsonl \
  --output training_data_with_category.jsonl
```

### Step 2: Run Benchmark and Train

> âœ… **Verify Candle is working:** If you see warnings like `"Candle embedding failed, falling back to hash-based"`,
> the Candle library is not properly loaded. Go back to Prerequisites and ensure `LD_LIBRARY_PATH` is set.

**Full training workflow (WSL example):**

```bash
# 1. First, set up Candle (do this once per terminal session)
cd /mnt/c/vllmproject/semantic-router/candle-binding
cargo build --release --no-default-features  # CPU-only, no CUDA needed
export LD_LIBRARY_PATH=$(pwd)/target/release:$LD_LIBRARY_PATH

# 2. Navigate to semantic-router
cd ../src/semantic-router

# 3. Run training with benchmark (queries all LLMs, generates embeddings)
go run ./cmd/vsr train --benchmark \
  --config ../../config/config-2-models-ollama.yaml \
  --query-limit 100 \
  --data-file pkg/modelselection/data/training_data_with_category.jsonl \
  --output-dir pkg/modelselection/data/trained_models
```

**Train from existing benchmark data (no LLM queries):**

```bash
# Skip --benchmark flag to use existing benchmark_training_data.jsonl
go run ./cmd/vsr train \
  --config ../../config/config-2-models-ollama.yaml \
  --data-file pkg/modelselection/data/benchmark_training_data.jsonl \
  --output-dir pkg/modelselection/data/trained_models
```

**Parameters:**

- `--benchmark`: Run benchmark to query all LLMs (generates `benchmark_training_data.jsonl`)
- `--config`: Path to config file with LLM definitions
- `--query-limit`: Number of queries to benchmark (Ã—4 models = total API calls)
- `--data-file`: Source queries with categories
- `--output-dir`: Where to save trained model JSON files
- `--embedding-model`: Embedding model to use: `qwen3` (default), `gemma`, or `bert`
- `--embedding-dim`: Embedding dimension (default: 768)
- `--use-fallback`: Use hash-based embeddings (**development only**, not recommended for production)
- `--analyze-only`: Only analyze config for multi-model categories, don't train
- `--concurrency`: Number of concurrent API requests (default: 1 for free tier, 4+ for paid APIs)
- `--rate-limit-delay`: Delay between requests in milliseconds (default: 500 for free tier, 0 for paid APIs)

**Algorithm Hyperparameters (Configurable via CLI):**

| Flag | Default | Description |
|------|---------|-------------|
| `--quality-weight` | 0.9 | **Global** quality vs speed weight for ALL algorithms (0=pure speed, 1=pure quality) |
| `--knn-k` | 5 | KNN: Number of neighbors to consider |
| `--kmeans-clusters` | 8 | KMeans: Number of clusters |
| `--mlp-hidden-layers` | "256,128" | MLP: Hidden layer sizes (comma-separated) |
| `--mf-num-factors` | 16 | Matrix Factorization: Number of latent factors |

**Quality Weight Examples:**

| `--quality-weight` | Behavior | Use Case |
|-------------------|----------|----------|
| 1.0 | 100% quality, ignore speed | Accuracy-critical applications |
| **0.9** (default) | 90% quality, 10% speed | Quality-first (recommended) |
| 0.7 | 70% quality, 30% speed | Balanced approach |
| 0.5 | 50% quality, 50% speed | Equal balance |
| 0.3 | 30% quality, 70% speed | Speed-first, cost-sensitive |
| 0.0 | 100% speed, ignore quality | Latency-critical applications |

**Example with custom hyperparameters:**

```bash
go run ./cmd/vsr train --benchmark \
  --config ../../config/config-2-models-ollama.yaml \
  --query-limit 100 \
  --quality-weight 0.8 \
  --knn-k 7 \
  --kmeans-clusters 12 \
  --mlp-hidden-layers "512,256,128" \
  --mf-num-factors 20
```

**Embedding Options:**

| Flag | Embedding | Requires Candle | Use Case |
|------|-----------|-----------------|----------|
| (default) | Qwen3 | âœ… Yes | Production training |
| `--embedding-model gemma` | Gemma | âœ… Yes | Faster training |
| `--use-fallback` | Hash-based | âŒ No | Testing/development |

**Time estimates:**

| Query Limit | Models | Total API Calls | Estimated Time |
|-------------|--------|-----------------|----------------|
| 25 | 4 | 100 | ~45 min |
| 50 | 4 | 200 | ~1.5 hours |
| 100 | 4 | 400 | ~3-4 hours |
| 400 | 4 | 1600 | ~12-15 hours |

### Step 3: Train on Existing Benchmark Data

If you already have benchmark data:

```bash
# With Qwen3 embeddings (production)
go run ./cmd/vsr train \
  --data-file pkg/modelselection/data/benchmark_training_data.jsonl \
  --output-dir pkg/modelselection/data/trained_models

# With hash-based fallback (testing/development)
go run ./cmd/vsr train --use-fallback \
  --data-file pkg/modelselection/data/benchmark_training_data.jsonl \
  --output-dir pkg/modelselection/data/trained_models
```

---

## Inference Flow

When a new query arrives:

```
1. Query: "What is the derivative of sin(x)?"
                    â”‚
                    â–¼
2. Category Classification
   â””â”€â”€ category: "math"
                    â”‚
                    â–¼
3. Generate Embedding (Candle/Qwen3)
   â””â”€â”€ embedding: [0.123, -0.456, ..., 0.789]  (768 dims)
                    â”‚
                    â–¼
4. Build Feature Vector
   â””â”€â”€ embedding + one_hot(category) = 782 dims
                    â”‚
                    â–¼
5. Load Algorithm (based on decision config)
   â””â”€â”€ KNN selector from knn_model.json
                    â”‚
                    â–¼
6. Run Inference
   â””â”€â”€ Find 5 nearest neighbors
   â””â”€â”€ Vote: 3Ã— llama-3.2-3b, 2Ã— llama-3.2-1b
                    â”‚
                    â–¼
7. Return Best Model
   â””â”€â”€ "llama-3.2-3b"
```

---

## Testing

### Test Files

| File | Purpose |
|------|---------|
| `selector_test.go` | Unit tests for algorithm implementations |
| `selector_load_test.go` | Generalization tests with pre-trained models |

### Run Generalization Tests (Recommended)

These tests load pre-trained models and test on NEW queries:

```bash
# On WSL (required for real Qwen3 embeddings)
export LD_LIBRARY_PATH=/mnt/c/vllmproject/semantic-router/candle-binding/target/release:$LD_LIBRARY_PATH
cd /mnt/c/vllmproject/semantic-router/src/semantic-router/pkg/modelselection

# Test all 14 VSR categories with all 5 algorithms
go test -v -run 'TestGeneralizationAllCategories' . -timeout 30m

# Analyze KNN voting behavior
go test -v -run 'TestKNNVotingAnalysis' . -timeout 5m

# Test training with BEST model per query (accuracy measurement)
go test -v -run 'TestTrainWithBestModelPerQuery' . -timeout 30m
```

### Run Unit Tests

```bash
cd src/semantic-router
go test -v ./pkg/modelselection/... -run TestLoadPretrainedSelectors
go test -v ./pkg/modelselection/... -run TestModelSelectionWithVariedQueries
```

### Test Methodology

The generalization tests in `selector_load_test.go` follow this flow:

```
1. Load pre-trained models from disk
   â””â”€â”€ knn_model.json, kmeans_model.json, etc.
                    â”‚
                    â–¼
2. Generate NEW embeddings for test queries using Qwen3
   â””â”€â”€ Queries are NOT in training data
                    â”‚
                    â–¼
3. Run inference with each algorithm
   â””â”€â”€ Track which model is selected per query
                    â”‚
                    â–¼
4. Analyze model distribution
   â””â”€â”€ Verify algorithms select MULTIPLE models (not just 1)
```

### Test Results

All 5 algorithms successfully:

1. Load from JSON files
2. Mark as `trained: true`
3. Select **multiple different models** based on query characteristics
4. Use the quality Ã— efficiency formula correctly

**Example test output (WSL with real Qwen3):**

```
=== RUN   TestGeneralizationAllCategories/knn
  [biology] 'Explain the structure of a eukaryotic cell' -> llama-3.2-3b
  [chemistry] 'Describe the structure of an atom' -> mistral-7b
  [computer science] 'Write a breadth-first search algorithm' -> llama-3.2-1b
  [math] 'Find the derivative of sin(x) * e^x' -> llama-3.2-1b
  [physics] 'Explain Maxwell's equations for electromagnetism' -> mistral-7b
  Overall model distribution: map[codellama-7b:1 llama-3.2-1b:11 llama-3.2-3b:8 mistral-7b:10]
  âœ“ knn selected 4 different models across categories
```

---

## Model Loading (Code Reference)

### Loading a Selector

```go
// persistence.go
func LoadSelectorFromJSON(algorithm, modelsPath string, refs []config.ModelRef) (ModelSelector, error) {
    filePath := filepath.Join(modelsPath, algorithm+"_model.json")
    data, err := os.ReadFile(filePath)
    if err != nil {
        return nil, err
    }
    
    var modelData ModelData
    json.Unmarshal(data, &modelData)
    
    selector := createSelector(algorithm)
    selector.LoadFromJSON(modelData)
    
    return selector, nil
}
```

### Selector Interface

```go
// selector.go
type ModelSelector interface {
    Select(ctx context.Context, query string, refs []config.ModelRef, 
           embedding []float64, category string) (config.ModelRef, error)
    Train(training []TrainingRecord) error
    LoadFromJSON(data ModelData) error
    SaveToJSON() (ModelData, error)
    IsTrained() bool
}
```

---

## Performance Considerations

| Algorithm | Training Speed | Inference Speed | Memory Usage |
|-----------|---------------|-----------------|--------------|
| KNN | Fast | O(n) - slower with more data | High (stores all embeddings) |
| KMeans | Medium | O(k) - fast | Low (only centroids) |
| MLP | Slow | O(1) - very fast | Medium (weights) |
| SVM | Slow | O(sv) - depends on support vectors | Medium |
| Matrix Factorization | Medium | O(1) - very fast | Low |

---

## Troubleshooting

### Model not trained (always selects first model)

- Check `trained: true` in JSON files
- Ensure enough training records (minimum ~20-50)
- Verify embeddings are being generated (check for Candle errors)

### Dimension mismatch errors

- Ensure embedding model outputs 768 dimensions
- Check category one-hot encoding produces 14 dimensions
- Total feature vector should be 782 dimensions

### Benchmark timeout errors

- Increase Ollama timeout in config
- Ensure models are loaded (first request is slow)
- Check available RAM for larger models

### Windows: Tests fail with dimension mismatch (384 vs 768)

On Windows, the Candle library uses a **mock implementation** that generates 384-dim hash-based 
embeddings instead of real 768-dim Qwen3 embeddings.

**Solution:** Run tests on WSL (Windows Subsystem for Linux):

```bash
# Set library path for Candle
export LD_LIBRARY_PATH=/mnt/c/vllmproject/semantic-router/candle-binding/target/release:$LD_LIBRARY_PATH

# Run tests
cd /mnt/c/vllmproject/semantic-router/src/semantic-router/pkg/modelselection
go test -v -run 'TestGeneralizationAllCategories' . -timeout 30m
```

The mock is triggered by this build constraint in `candle-binding/semantic-router_mock.go`:

```go
//go:build windows || !cgo
```

---

## Algorithm Accuracy & Benchmark Results

### Test Methodology

The tests in `selector_load_test.go` follow this methodology:

1. **Load pre-trained models from disk** (not re-train)
2. **Generate NEW embeddings** for test queries using Qwen3
3. **Test on queries NOT in training data** (generalization test)

This ensures we're testing the model's ability to generalize to unseen queries.

### Test Results Summary

#### TestGeneralizationAllCategories (30 NEW queries, 14 categories)

| Algorithm | Implementation | Linfa Crate | Models Selected | Model Distribution | Success |
|-----------|----------------|-------------|-----------------|-------------------|---------|
| **KNN** | Rust | `linfa-nn` | **4** | codellama:4, llama-1b:13, llama-3b:4, mistral:9 | 30/30 âœ… |
| **KMeans** | Rust | `linfa-clustering` | **1** | mistral:30 (cluster-based) | 30/30 âœ… |
| **MLP** | Go | N/A | **4** | codellama:4, llama-1b:12, llama-3b:7, mistral:7 | 30/30 âœ… |
| **SVM** | Rust | `linfa-svm` | **4** | codellama:4, llama-1b:13, llama-3b:3, mistral:10 | 30/30 âœ… |
| **MF** | Go | N/A | **3** | llama-1b:8, llama-3b:9, mistral:13 | 30/30 âœ… |

#### Model Selection Diversity Chart

```
KNN     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ codellama (13%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-1b (43%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-3b (13%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ mistral (30%)

KMeans  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ codellama (0%)
        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-1b (0%)
        â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-3b (0%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ mistral (100%)

MLP     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ codellama (13%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-1b (40%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-3b (23%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ mistral (23%)

SVM     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ codellama (13%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-1b (43%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-3b (10%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ mistral (33%)

MF      â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ codellama (0%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-1b (27%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ llama-3b (30%)
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ mistral (43%)
```

#### Key Insights

| Metric | Best Algorithm |
|--------|----------------|
| **Most Diverse** | KNN, SVM, MLP (use all 4 models) |
| **Fastest Inference** | KMeans, MLP (< 1s for 30 queries) |
| **Best for Quality** | KNN (Ball Tree nearest neighbor) |
| **Cluster-based** | KMeans (routes to dominant cluster) |
| **Decision Boundaries** | SVM (RBF kernel) |

### Benchmark Results (WSL with Real Qwen3)

**Ground Truth Distribution (100 queries, 4 LLMs):**

| Model | Queries where BEST | Percentage |
|-------|-------------------|------------|
| codellama-7b | 74 | 74% |
| llama-3.2-3b | 22 | 22% |
| mistral-7b | 8 | 8% |
| llama-3.2-1b | 1 | 1% |

**Algorithm Accuracy (Train with BEST model per query):**

| Algorithm | Accuracy | Notes |
|-----------|----------|-------|
| **KMeans** | 55% | Best overall |
| **MLP** | 55% | Best overall |
| **SVM** | 55% | Best overall |
| KNN | 35% | Lower due to voting among duplicates |
| MF | 15% | Collaborative filtering approach |

All algorithms now correctly select **multiple different models** based on query characteristics.

### RouteLLM Alignment

The implementation is aligned with [RouteLLM (arXiv:2406.18665)](https://arxiv.org/abs/2406.18665):

| RouteLLM Concept | Our Implementation |
|------------------|-------------------|
| Routes between stronger/weaker LLMs | Selects from multiple LLMs |
| Optimizes cost vs quality | Formula: `quality Ã— efficiency` |
| Uses preference data for training | `TrainWithPreferences()` + BPR |
| Matrix Factorization router | `MatrixFactorizationSelector` |
| Transfer learning capability | Pre-trained models generalize to new queries |
