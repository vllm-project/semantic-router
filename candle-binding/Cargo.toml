[package]
name = "candle-semantic-router"
version = "0.4.0"
edition = "2021"
description = "Go bindings for Candle BERT semantic similarity model for LLM routing"
license = "MIT OR Apache-2.0"

[lib]
name = "candle_semantic_router"
# Order: rlib for Rust-to-Rust linking, staticlib for C/Go FFI (static), cdylib for dynamic linking
crate-type = ["rlib", "staticlib", "cdylib"]

[features]
default = ["cuda"]
# CUDA support (enables GPU acceleration)
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
# Flash Attention 2 support (requires CUDA and compatible GPU)
# Enable with: cargo build --features flash-attn
# Note: Requires CUDA Compute Capability >= 8.0 (Ampere or newer)
flash-attn = ["cuda", "candle-flash-attn"]

[dependencies]
anyhow = { version = "1", features = ["backtrace"] }
candle-core = { git = "https://github.com/huggingface/candle", tag = "0.9.2-alpha.1" }
candle-nn = { git = "https://github.com/huggingface/candle", tag = "0.9.2-alpha.1" }
candle-transformers = { git = "https://github.com/huggingface/candle", tag = "0.9.2-alpha.1" }
# Flash Attention 2 (optional, requires CUDA)
# Reference: https://github.com/huggingface/candle/tree/main/candle-flash-attn
# Using git dependency to automatically fetch CUTLASS submodule
candle-flash-attn = { git = "https://github.com/huggingface/candle", tag = "0.9.2-alpha.1", optional = true }
tokenizers = { version = "0.21.0", default-features = false, features = ["http", "onig"] }
hf-hub = { version = "0.4.1", default-features = false, features = ["rustls-tls"] }
safetensors = "0.4.1"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0.93"
tracing = "0.1.37"
libc = "0.2.147"
rand = "0.8.5"
# Performance optimization: parallel processing and lock-free initialization
rayon = "1.8"
parking_lot = "0.12"
crossbeam-channel = "0.5"  # Efficient multi-channel select for scheduler wakeup

[dev-dependencies]
rstest = "0.18"
tokio = { version = "1.0", features = ["full"] }
tempfile = "3.8"
serial_test = "3.0"
criterion = "0.5"
async-std = { version = "1.12", features = ["attributes"] }

# Example demonstrating Qwen3 Multi-LoRA classification
[[example]]
name = "qwen3_example"
path = "../deploy/examples/candle-binding/qwen3_example.rs"

# Example demonstrating DeBERTa v2 Prompt Injection Detection
[[example]]
name = "deberta_prompt_injection_example"
path = "../deploy/examples/candle-binding/deberta_prompt_injection_example.rs"

# Example showing raw softmax confidence values
[[example]]
name = "test_raw_confidence"
path = "../deploy/examples/candle-binding/test_raw_confidence.rs"

# Example testing LoRA adapter compatibility with ModernBERT-base-32k
[[example]]
name = "test_lora_compatibility_32k"
path = "examples/test_lora_compatibility_32k.rs"

# Example testing full inference with LoRA adapters on ModernBERT-base-32k
[[example]]
name = "test_lora_inference_32k"
path = "examples/test_lora_inference_32k.rs"

# Example listing safetensors keys
[[example]]
name = "list_safetensors_keys"
path = "examples/list_safetensors_keys.rs"

# Example testing long text inference (8K, 16K, 32K tokens)
[[example]]
name = "test_long_text_inference_32k"
path = "examples/test_long_text_inference_32k.rs"

# Example testing ModernBERT-base-32k comprehensive validation
[[example]]
name = "test_modernbert_32k_validation"
path = "examples/test_modernbert_32k_validation.rs"

# Example testing classifier compatibility with Extended32K (Phase 1.5)
[[example]]
name = "test_classifier_compatibility"
path = "examples/test_classifier_compatibility.rs"

# Example testing full inference with Extended32K base model + classifier (Phase 1.5/2)
[[example]]
name = "test_full_inference_32k"
path = "examples/test_full_inference_32k.rs"

# Example testing 32K classifier inference (Phase 1.5/2)
[[example]]
name = "test_32k_classifier_inference"
path = "examples/test_32k_classifier_inference.rs"

# Example for performance benchmarking (Phase 6)
[[example]]
name = "benchmark_performance"
path = "examples/benchmark_performance.rs"

# Example for concurrent request benchmarking (Phase 6 - Chunking Threshold Analysis)
[[example]]
name = "benchmark_concurrent"
path = "examples/benchmark_concurrent.rs"

# Example for embedding performance benchmarking
[[example]]
name = "embedding_benchmark"
path = "examples/embedding_benchmark.rs"

# Example testing mmBERT LoRA merged models
[[example]]
name = "test_mmbert_lora"
path = "examples/test_mmbert_lora.rs"

# Example testing real mmBERT model loading and inference
[[example]]
name = "test_mmbert_real"
path = "examples/test_mmbert_real.rs"

# Example testing mmBERT variant detection
[[example]]
name = "test_mmbert_variant"
path = "examples/test_mmbert_variant.rs"

# Note: Benchmark binaries are located in ../bench/scripts/rust/candle-binding/
# They are not included in the library build to keep it self-contained.
# To run benchmarks, use the workspace-level Cargo.toml or run them directly from the bench directory.
