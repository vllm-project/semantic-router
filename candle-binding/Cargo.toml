[package]
name = "candle-semantic-router"
version = "0.4.0"
edition = "2021"
description = "Go bindings for Candle BERT semantic similarity model for LLM routing"
license = "MIT OR Apache-2.0"

[lib]
name = "candle_semantic_router"
crate-type = ["staticlib", "cdylib"]

[features]
default = []
# CUDA support (enables GPU acceleration)
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
# Flash Attention 2 support (requires CUDA and compatible GPU)
# Enable with: cargo build --features flash-attn
# Note: Requires CUDA Compute Capability >= 8.0 (Ampere or newer)
flash-attn = ["cuda", "candle-flash-attn"]

[dependencies]
anyhow = { version = "1", features = ["backtrace"] }
candle-core = "0.8.4"
candle-nn = "0.8.4"
candle-transformers = "0.8.4"
# Flash Attention 2 (optional, requires CUDA)
# Reference: https://github.com/huggingface/candle/tree/main/candle-flash-attn
candle-flash-attn = { version = "0.8.4", optional = true }
tokenizers = { version = "0.21.0", features = ["http"] }
hf-hub = "0.4.1"
safetensors = "0.4.1"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0.93"
tracing = "0.1.37"
libc = "0.2.147"
lazy_static = "1.4.0"
rand = "0.8.5"
# Performance optimization: parallel processing and lock-free initialization
rayon = "1.8"       
once_cell = "1.19"

[dev-dependencies]
rstest = "0.18"
tokio = { version = "1.0", features = ["full"] }
tempfile = "3.8"
serial_test = "3.0"
criterion = "0.5"
async-std = { version = "1.12", features = ["attributes"] } 
