# Semantic Router Configuration for MCP E2E Tests
# This configuration enables MCP (Model Context Protocol) classification

config:
  bert_model:
    model_id: models/mom-embedding-light
    threshold: 0.6
    use_cpu: true

  semantic_cache:
    enabled: true
    backend_type: "memory"
    similarity_threshold: 0.8
    max_entries: 1000
    ttl_seconds: 3600
    eviction_policy: "fifo"
    use_hnsw: true
    hnsw_m: 16
    hnsw_ef_construction: 200
    embedding_model: "bert"

  tools:
    enabled: true
    top_k: 3
    similarity_threshold: 0.2
    tools_db_path: "config/tools_db.json"
    fallback_to_empty: true

  prompt_guard:
    enabled: true
    use_modernbert: false  # MoM uses LoRA-based model
    model_id: "models/mom-jailbreak-classifier"
    threshold: 0.7
    use_cpu: true
    jailbreak_mapping_path: "models/mom-jailbreak-classifier/jailbreak_type_mapping.json"

  # Classifier configuration with MCP enabled
  classifier:
    # MCP category model configuration
    mcp_category_model:
      enabled: true
      transport_type: "stdio"  # Options: "stdio" or "http"
      # For stdio transport:
      command: "python3"
      args: ["deploy/mcp-classifier-server/server_keyword.py"]
      # For HTTP transport (alternative):
      # transport_type: "http"
      # url: "http://localhost:8090/mcp"
      threshold: 0.6
      timeout_seconds: 30
      tool_name: "classify_text"  # Optional: MCP tool name for classification

    # Fallback to in-tree classifiers if MCP fails
    category_model:
      model_id: "models/mom-domain-classifier"
      use_modernbert: false  # MoM uses LoRA-based model
      threshold: 0.6
      use_cpu: true
      category_mapping_path: "models/mom-domain-classifier/category_mapping.json"

    pii_model:
      model_id: "models/pii_classifier_modernbert-base_presidio_token_model"
      use_modernbert: true
      threshold: 0.7
      use_cpu: true
      pii_mapping_path: "models/mom-pii-classifier/pii_type_mapping.json"

  # Categories will be loaded dynamically from MCP server via list_categories tool
  # These are fallback categories if MCP is unavailable
  categories:
    - name: math
      description: "Mathematics and quantitative reasoning"
      mmlu_categories: ["math"]
    - name: science
      description: "Science and natural sciences"
      mmlu_categories: ["physics", "chemistry", "biology"]
    - name: technology
      description: "Technology and computer science"
      mmlu_categories: ["computer_science"]
    - name: history
      description: "History and cultural topics"
      mmlu_categories: ["history"]
    - name: general
      description: "General knowledge and miscellaneous topics"
      mmlu_categories: ["other"]

  strategy: "priority"

  vllm_endpoints: []

  model_config:
    # Model for MCP-recommended routing
    openai/gpt-oss-20b:
      name: "openai/gpt-oss-20b"
      family: "gpt-oss"
      supports_reasoning: true
      preferred_endpoints:
        - address: "demo-llm-service.default.svc.cluster.local"
          port: 8000
          weight: 100

  decisions:
    - name: "math_decision"
      description: "Mathematics and quantitative reasoning"
      priority: 100
      rules:
        operator: "AND"
        conditions:
          - type: "domain"
            name: "math"
      modelRefs:
        - model: "openai/gpt-oss-20b"
          use_reasoning: false  # Can be overridden by MCP
      plugins:
        - type: "system_prompt"
          configuration:
            system_prompt: "You are a mathematics expert. Provide step-by-step solutions with clear explanations. Show your work and verify calculations."
        - type: "pii"
          configuration:
            enabled: true
            pii_types_allowed: []

    - name: "science_decision"
      description: "Science and natural sciences"
      priority: 100
      rules:
        operator: "AND"
        conditions:
          - type: "domain"
            name: "science"
      modelRefs:
        - model: "openai/gpt-oss-20b"
          use_reasoning: false
      plugins:
        - type: "system_prompt"
          configuration:
            system_prompt: "You are a science expert. Explain scientific concepts clearly and accurately. Use examples and analogies when helpful."
        - type: "pii"
          configuration:
            enabled: true
            pii_types_allowed: []

    - name: "technology_decision"
      description: "Technology and computer science"
      priority: 100
      rules:
        operator: "AND"
        conditions:
          - type: "domain"
            name: "technology"
      modelRefs:
        - model: "openai/gpt-oss-20b"
          use_reasoning: false
      plugins:
        - type: "system_prompt"
          configuration:
            system_prompt: "You are a technology and programming expert. Provide practical solutions with code examples when appropriate."
        - type: "pii"
          configuration:
            enabled: true
            pii_types_allowed: []

    - name: "history_decision"
      description: "History and cultural topics"
      priority: 100
      rules:
        operator: "AND"
        conditions:
          - type: "domain"
            name: "history"
      modelRefs:
        - model: "openai/gpt-oss-20b"
          use_reasoning: false
      plugins:
        - type: "system_prompt"
          configuration:
            system_prompt: "You are a history expert. Provide accurate historical information with context and relevant details."
        - type: "pii"
          configuration:
            enabled: true
            pii_types_allowed: []

    - name: "general_decision"
      description: "General knowledge and miscellaneous topics"
      priority: 50
      rules:
        operator: "AND"
        conditions:
          - type: "domain"
            name: "general"
      modelRefs:
        - model: "openai/gpt-oss-20b"
          use_reasoning: false
      plugins:
        - type: "system_prompt"
          configuration:
            system_prompt: "You are a helpful and knowledgeable assistant. Provide accurate, helpful responses across a wide range of topics."
        - type: "semantic-cache"
          configuration:
            enabled: true
            similarity_threshold: 0.75
        - type: "pii"
          configuration:
            enabled: true
            pii_types_allowed: []

  default_model: openai/gpt-oss-20b

  # Reasoning family configurations
  reasoning_families:
    deepseek:
      type: "chat_template_kwargs"
      parameter: "thinking"
    qwen3:
      type: "chat_template_kwargs"
      parameter: "enable_thinking"
    gpt-oss:
      type: "reasoning_effort"
      parameter: "reasoning_effort"
    gpt:
      type: "reasoning_effort"
      parameter: "reasoning_effort"

  default_reasoning_effort: high

  # API Configuration
  api:
    batch_classification:
      max_batch_size: 100
      concurrency_threshold: 5
      max_concurrency: 8
      metrics:
        enabled: true
        detailed_goroutine_tracking: true
        high_resolution_timing: false
        sample_rate: 1.0
        duration_buckets:
          [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30]
        size_buckets: [1, 2, 5, 10, 20, 50, 100, 200]

  # Embedding Models Configuration
  embedding_models:
    qwen3_model_path: "models/mom-embedding-pro"
    # gemma_model_path: "models/mom-embedding-flash"
    use_cpu: true

  # Observability Configuration
  observability:
    tracing:
      enabled: true
      provider: "opentelemetry"
      exporter:
        type: "otlp"
        endpoint: "jaeger:4317"
        insecure: true
      sampling:
        type: "always_on"
        rate: 1.0
      resource:
        service_name: "vllm-semantic-router"
        service_version: "v0.1.0"
        deployment_environment: "development"
