# Router config for Hierarchical + Hybrid Memory Retrieval Test
#
# Native Go RouterConfig format (not vllm-sr CLI format).
# Used directly by: ./bin/router -config=config/testing/config.memory-hierarchical.yaml
#
# Features enabled:
#   - hierarchical_search: two-phase category-then-drilldown retrieval
#   - hybrid_search: fused BM25 + n-gram + vector scoring
#   - auto_store: memories extracted from conversations
#
# Prerequisites:
#   - Milvus on localhost:19530 (make start-milvus)
#   - Echo mock vLLM on 127.0.0.1:8003
#   - BERT model downloaded (make download-models)
#
# Usage:
#   make test-retrieval-api  (starts everything automatically)
#   OR manually:
#     make run-router-memory-hierarchical  (starts router only)
#     ./scripts/test-retrieval-api.sh      (runs tests)

response_api:
  enabled: true
  store_backend: memory
  ttl_seconds: 86400
  max_responses: 1000

semantic_cache:
  enabled: false

prompt_guard:
  enabled: false

classifier:
  category_model:
    model_id: "models/mom-embedding-light"
    threshold: 0.5
    use_cpu: true

bert_model:
  model_id: "models/mom-embedding-light"
  threshold: 0.5
  use_cpu: true

embedding_models:
  bert_model_path: "models/mom-embedding-light"
  use_cpu: true

categories:
  - name: other
    description: "General knowledge and miscellaneous topics"
    mmlu_categories: ["other"]

decisions:
  - name: "default_route"
    description: "Default route for hierarchical + hybrid memory testing"
    priority: 1
    rules:
      operator: "OR"
      conditions:
        - type: "domain"
          name: "other"
    modelRefs:
      - model: "qwen3"
        use_reasoning: false
    plugins:
      - type: "system_prompt"
        configuration:
          system_prompt: "You are MoM, a helpful AI assistant with memory."
          mode: "insert"
      - type: "memory"
        configuration:
          enabled: true
          retrieval_limit: 10
          similarity_threshold: 0.30
          auto_store: true
          hierarchical_search: true
          max_depth: 3
          hybrid_search: true
          hybrid_mode: "weighted"

vllm_endpoints:
  - name: "echo_vllm"
    address: "127.0.0.1"
    port: 8003
    weight: 1
    protocol: "http"
    model: "qwen3"

model_config:
  qwen3:
    preferred_endpoints: ["echo_vllm"]

default_model: "qwen3"

strategy: "priority"

memory:
  enabled: true
  auto_store: true
  milvus:
    address: "localhost:19530"
    collection: "memory_hierarchical_test"
    dimension: 384
  embedding_model: "bert"
  default_retrieval_limit: 10
  default_similarity_threshold: 0.30
  extraction_batch_size: 1

external_models:
  - llm_provider: "vllm"
    model_role: "memory_rewrite"
    llm_endpoint:
      address: "127.0.0.1"
      port: 8003
    llm_model_name: "qwen3"
    llm_timeout_seconds: 30
  - llm_provider: "vllm"
    model_role: "memory_extraction"
    llm_endpoint:
      address: "127.0.0.1"
      port: 8003
    llm_model_name: "qwen3"
    llm_timeout_seconds: 30
