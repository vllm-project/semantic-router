# Modality Routing Config - Auto-route between AR (text) and Diffusion (image) models
#
# Uses modality as a SIGNAL in the decision engine, composable with all other signals
# (keyword, domain, language, context, latency, etc.).
#
# Supports three detection methods for modality classification:
#   - "classifier":  ML-based (mmBERT-32K) — 3-class: AR / DIFFUSION / BOTH
#   - "keyword":     Keyword pattern matching — configurable keywords + both_keywords
#   - "hybrid":      Classifier + keyword fallback/confirmation
#
# Architecture:
#   User Prompt → Signal Evaluation (modality + other signals) → Decision Engine → Execution
#     - AR:        Normal LLM routing (passthrough)
#     - DIFFUSION: Image generation via diffusion endpoint
#     - BOTH:      AR text + image generation in parallel
#
# Run with: CONFIG_FILE=config/testing/config.modality-routing.yaml make run-router

# Disable features not needed for this demo
semantic_cache:
  enabled: false

tools:
  enabled: false

prompt_guard:
  enabled: false

# Response API for testing
response_api:
  enabled: true
  store_backend: "memory"
  ttl_seconds: 3600
  max_responses: 100

# vLLM endpoints - AR text model + Diffusion image model
vllm_endpoints:
  - name: "vllm-ar"
    address: "127.0.0.1"
    port: 8000
    weight: 1
    health_check_path: "/health"
  - name: "vllm-omni"
    address: "127.0.0.1"
    port: 8001
    weight: 1
    health_check_path: "/health"

# Model config - map models to their preferred endpoints
model_config:
  "Qwen/Qwen2.5-14B-Instruct":
    reasoning_family: "none"
    preferred_endpoints: ["vllm-ar"]
  "Qwen/Qwen-Image":
    reasoning_family: "none"
    preferred_endpoints: ["vllm-omni"]

# ── Modality Routing (detection + image generation config) ──────────────────
# The detection config is used by the modality signal evaluator.
# The image_gen config is used when a DIFFUSION or BOTH decision executes.
modality_routing:
  enabled: true
  ar_model: "Qwen/Qwen2.5-14B-Instruct"
  ar_endpoint: "http://localhost:8000/v1"
  diffusion_model: "Qwen/Qwen-Image"
  diffusion_endpoint: "http://localhost:8001/v1"

  image_gen:
    backend: "vllm_omni"
    default_width: 1024
    default_height: 1024
    timeout_seconds: 120
    response_text: "Here is the generated image based on your request."
    prompt_prefixes:
      - "generate an image of "
      - "create an image of "
      - "draw an image of "
      - "make an image of "
      - "generate a picture of "
      - "create a picture of "
      - "draw a picture of "
      - "generate "
      - "create "
      - "draw "
    backend_config:
      base_url: "http://localhost:8001"
      model: "Qwen/Qwen-Image"
      num_inference_steps: 4
      cfg_scale: 0.0

  detection:
    method: "hybrid"

    classifier:
      model_path: "./models/mmbert32k-modality-router-merged"
      use_cpu: false

    confidence_threshold: 0.6
    lower_threshold_ratio: 0.7

    keywords:
      - "generate an image"
      - "generate image"
      - "create an image"
      - "create image"
      - "draw"
      - "make a picture"
      - "generate a picture"
      - "create a picture"
      - "make an image"
      - "paint"
      - "sketch"
      - "render an image"
      - "produce an image"

    both_keywords:
      - "and illustrate"
      - "with a picture"
      - "with an image"
      - "with an illustration"
      - "with a diagram"
      - "include a picture"
      - "include an image"
      - "and generate an image"
      - "and draw"
      - "also generate"
      - "with a photo"

# ── Modality Signal Rules ──────────────────────────────────────────────────
# Defines signal names for modality classification (like fact_check_rules, user_feedback_rules)
# The classifier produces one of these signal names, which decisions reference via type: "modality"
modality_rules:
  - name: "AR"
    description: "Text-only response via autoregressive LLM"
  - name: "DIFFUSION"
    description: "Image generation via diffusion model"
  - name: "BOTH"
    description: "Hybrid response requiring both text and image"

# Minimal categories (required by schema)
categories:
  - name: text_generation
    description: "Text generation and general queries"
    mmlu_categories: ["other"]

strategy: "priority"

# ── Decisions ─────────────────────────────────────────────────────────────
# Modality is now a signal, composed with other signals in the decision engine.
# This enables combining modality with keyword, domain, language, context, etc.
decisions:
  - name: "image_generation"
    description: "Route image generation requests to diffusion model"
    priority: 200
    rules:
      operator: "AND"
      conditions:
        - type: "modality"
          name: "DIFFUSION"
    modelRefs:
      - model: "Qwen/Qwen-Image"
        use_reasoning: false

  - name: "text_and_image"
    description: "Route hybrid text+image requests to both models in parallel"
    priority: 190
    rules:
      operator: "AND"
      conditions:
        - type: "modality"
          name: "BOTH"
    modelRefs:
      - model: "Qwen/Qwen2.5-14B-Instruct"
        use_reasoning: false

  - name: "text_generation"
    description: "Default text routing for AR (text-only) requests"
    priority: 1
    rules:
      operator: "OR"
      conditions:
        - type: "modality"
          name: "AR"
        - type: "domain"
          name: "text_generation"
    modelRefs:
      - model: "Qwen/Qwen2.5-14B-Instruct"
        use_reasoning: false

# Default model - AR text model handles unmatched requests (fallback)
default_model: "Qwen/Qwen2.5-14B-Instruct"

# Disable reasoning
reasoning_families: {}

# Minimal API config
api:
  batch_classification:
    max_batch_size: 10
    concurrency_threshold: 2
    max_concurrency: 4

# Observability - metrics only
observability:
  metrics:
    enabled: true
  tracing:
    enabled: false
