# Recipe: Token Efficiency-Optimized Configuration
# Objective: Minimize token usage and reduce costs
# Trade-offs: May sacrifice some accuracy, uses aggressive caching
# Use case: High-volume production deployments, cost-sensitive applications
#
# Key optimizations:
# - Reasoning disabled for most categories (reduces token usage)
# - Low reasoning effort when reasoning is needed
# - Aggressive semantic caching (high similarity threshold, long TTL)
# - Lower classification thresholds for faster routing
# - Reduced tool selection (fewer tool tokens)
# - Relaxed PII policies (less token overhead)
# - Larger batch sizes for efficient processing

bert_model:
  model_id: models/all-MiniLM-L12-v2
  threshold: 0.5  # Lower threshold for faster matching
  use_cpu: true

semantic_cache:
  enabled: true  # Enable aggressive caching
  backend_type: "memory"
  similarity_threshold: 0.75  # Lower threshold for more cache hits
  max_entries: 10000  # Large cache for better hit rate
  ttl_seconds: 7200  # Long TTL (2 hours)
  eviction_policy: "lru"  # Keep most used entries

tools:
  enabled: true
  top_k: 1  # Select fewer tools to reduce tokens
  similarity_threshold: 0.3  # Higher threshold for stricter tool selection
  tools_db_path: "config/tools_db.json"
  fallback_to_empty: true

prompt_guard:
  enabled: true
  use_modernbert: true
  model_id: "models/jailbreak_classifier_modernbert-base_model"
  threshold: 0.75  # Higher threshold (less sensitive, fewer rejections)
  use_cpu: true
  jailbreak_mapping_path: "models/jailbreak_classifier_modernbert-base_model/jailbreak_type_mapping.json"

vllm_endpoints:
  - name: "endpoint1"
    address: "127.0.0.1"
    port: 8000
    weight: 1

model_config:
  "openai/gpt-oss-20b":
    reasoning_family: "gpt-oss"
    preferred_endpoints: ["endpoint1"]
    pii_policy:
      allow_by_default: true  # Relaxed PII policy for efficiency; when true, all PII types are allowed
    pricing:
      currency: USD
      prompt_per_1m: 0.10
      completion_per_1m: 0.30

classifier:
  category_model:
    model_id: "models/category_classifier_modernbert-base_model"
    use_modernbert: true
    threshold: 0.5  # Lower threshold for faster classification
    use_cpu: true
    category_mapping_path: "models/category_classifier_modernbert-base_model/category_mapping.json"
  pii_model:
    model_id: "models/pii_classifier_modernbert-base_presidio_token_model"
    use_modernbert: true
    threshold: 0.8  # Higher threshold (less sensitive, allows more content)
    use_cpu: true
    pii_mapping_path: "models/pii_classifier_modernbert-base_presidio_token_model/pii_type_mapping.json"

categories:
  - name: business
    system_prompt: "You are a business consultant. Provide concise, practical advice."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false  # Disable reasoning to save tokens
  - name: law
    system_prompt: "You are a legal information expert. Provide concise legal information."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.5
        use_reasoning: false
  - name: psychology
    system_prompt: "You are a psychology expert. Provide evidence-based insights."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.6
        use_reasoning: false
  - name: biology
    system_prompt: "You are a biology expert. Explain biological concepts clearly."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.8
        use_reasoning: false
  - name: chemistry
    system_prompt: "You are a chemistry expert. Provide clear explanations."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false  # Disable reasoning for token efficiency
  - name: history
    system_prompt: "You are a historian. Provide accurate historical context."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false
  - name: other
    system_prompt: "You are a helpful assistant. Provide concise, accurate responses."
    # Category-level cache (optional, already enabled globally)
    # semantic_cache_enabled: true
    # semantic_cache_similarity_threshold: 0.7  # Match global or slightly lower
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false
  - name: health
    system_prompt: "You are a health information expert. Provide evidence-based health information."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.6
        use_reasoning: false
  - name: economics
    system_prompt: "You are an economics expert. Provide data-driven economic insights."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.9
        use_reasoning: false
  - name: math
    system_prompt: "You are a mathematics expert. Provide clear, step-by-step solutions."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 1.0
        use_reasoning: true  # Only enable for math where reasoning is critical
  - name: physics
    system_prompt: "You are a physics expert. Explain physical concepts clearly."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.8
        use_reasoning: false  # Disable to save tokens
  - name: computer science
    system_prompt: "You are a computer science expert. Provide practical code solutions."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false
  - name: philosophy
    system_prompt: "You are a philosophy expert. Present clear philosophical perspectives."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.6
        use_reasoning: false
  - name: engineering
    system_prompt: "You are an engineering expert. Provide practical engineering solutions."
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.8
        use_reasoning: false

default_model: openai/gpt-oss-20b

reasoning_families:
  deepseek:
    type: "chat_template_kwargs"
    parameter: "thinking"
  qwen3:
    type: "chat_template_kwargs"
    parameter: "enable_thinking"
  gpt-oss:
    type: "reasoning_effort"
    parameter: "reasoning_effort"
  gpt:
    type: "reasoning_effort"
    parameter: "reasoning_effort"

default_reasoning_effort: low  # Minimal reasoning effort to save tokens

api:
  batch_classification:
    max_batch_size: 100  # Larger batches for efficiency
    concurrency_threshold: 10
    max_concurrency: 16  # Higher concurrency for throughput
    metrics:
      enabled: true
      detailed_goroutine_tracking: false  # Disable for efficiency
      high_resolution_timing: false
      sample_rate: 0.1  # Sample 10% to reduce overhead
      duration_buckets: [0.01, 0.05, 0.1, 0.5, 1, 5, 10]
      size_buckets: [1, 10, 50, 100, 200]

observability:
  tracing:
    enabled: true
    provider: "opentelemetry"
    exporter:
      type: "otlp"
      endpoint: "localhost:4317"
      insecure: true
    sampling:
      type: "probabilistic"
      rate: 0.1  # Sample 10% of traces to reduce overhead
    resource:
      service_name: "vllm-semantic-router-token-efficient"
      service_version: "v0.1.0"
      deployment_environment: "production"
