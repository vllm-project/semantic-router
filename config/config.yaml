bert_model:
  model_id: sentence-transformers/all-MiniLM-L12-v2
  threshold: 0.6
  use_cpu: true

semantic_cache:
  enabled: true
  backend_type: "memory"  # Options: "memory" or "milvus"
  similarity_threshold: 0.8
  max_entries: 1000  # Only applies to memory backend
  ttl_seconds: 3600
  eviction_policy: "fifo"  

tools:
  enabled: true
  top_k: 3
  similarity_threshold: 0.2
  tools_db_path: "config/tools_db.json"
  fallback_to_empty: true

prompt_guard:
  enabled: true
  use_modernbert: true
  model_id: "models/jailbreak_classifier_modernbert-base_model"
  threshold: 0.7
  use_cpu: true
  jailbreak_mapping_path: "models/jailbreak_classifier_modernbert-base_model/jailbreak_type_mapping.json"

# vLLM Endpoints Configuration
# IMPORTANT: 'address' field must be a valid IP address (IPv4 or IPv6)
# Supported formats: 127.0.0.1, 192.168.1.1, ::1, 2001:db8::1
# NOT supported: domain names (example.com), protocol prefixes (http://), paths (/api), ports in address (use 'port' field)
vllm_endpoints:
  - name: "endpoint1"
    address: "127.0.0.1"  # IPv4 address - REQUIRED format
    port: 8000
    models:
      - "openai/gpt-oss-20b"
    weight: 1

model_config:
  "openai/gpt-oss-20b":
    reasoning_family: "gpt-oss"  # This model uses GPT-OSS reasoning syntax
    preferred_endpoints: ["endpoint1"]
    pii_policy:
      allow_by_default: true

# Classifier configuration
classifier:
  category_model:
    model_id: "models/category_classifier_modernbert-base_model"
    use_modernbert: true
    threshold: 0.6
    use_cpu: true
    category_mapping_path: "models/category_classifier_modernbert-base_model/category_mapping.json"
  pii_model:
    model_id: "models/pii_classifier_modernbert-base_presidio_token_model"
    use_modernbert: true
    threshold: 0.7
    use_cpu: true
    pii_mapping_path: "models/pii_classifier_modernbert-base_presidio_token_model/pii_type_mapping.json"

# Categories with new use_reasoning field structure
categories:
  - name: business
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false  # Business performs better without reasoning
  - name: law
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.4
        use_reasoning: false
  - name: psychology
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.6
        use_reasoning: false
  - name: biology
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.9
        use_reasoning: false
  - name: chemistry
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.6
        use_reasoning: true  # Enable reasoning for complex chemistry
  - name: history
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false
  - name: other
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false
  - name: health
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.5
        use_reasoning: false
  - name: economics
    model_scores:
      - model: openai/gpt-oss-20b
        score: 1.0
        use_reasoning: false
  - name: math
    model_scores:
      - model: openai/gpt-oss-20b
        score: 1.0
        use_reasoning: true  # Enable reasoning for complex math
  - name: physics
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: true  # Enable reasoning for physics
  - name: computer science
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.6
        use_reasoning: false
  - name: philosophy
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.5
        use_reasoning: false
  - name: engineering
    model_scores:
      - model: openai/gpt-oss-20b
        score: 0.7
        use_reasoning: false

# Router Configuration for Dual-Path Selection
router:
  # High confidence threshold for automatic LoRA selection
  high_confidence_threshold: 0.99
  # Low latency threshold in milliseconds for LoRA path selection
  low_latency_threshold_ms: 2000
  # Baseline scores for path evaluation
  lora_baseline_score: 0.8
  traditional_baseline_score: 0.7
  embedding_baseline_score: 0.75
  # Success rate calculation threshold
  success_confidence_threshold: 0.8
  # Large batch size threshold for parallel processing
  large_batch_threshold: 4
  # Default performance metrics (milliseconds)
  lora_default_execution_time_ms: 1345
  traditional_default_execution_time_ms: 4567
  # Default processing requirements
  default_confidence_threshold: 0.95
  default_max_latency_ms: 5000
  default_batch_size: 4
  default_avg_execution_time_ms: 3000
  # Default confidence and success rates
  lora_default_confidence: 0.99
  traditional_default_confidence: 0.95
  lora_default_success_rate: 0.98
  traditional_default_success_rate: 0.95
  # Scoring weights for intelligent path selection (balanced approach)
  multi_task_lora_weight: 0.30        # LoRA advantage for multi-task processing
  single_task_traditional_weight: 0.30 # Traditional advantage for single tasks
  large_batch_lora_weight: 0.25       # LoRA advantage for large batches (≥4)
  small_batch_traditional_weight: 0.25 # Traditional advantage for single items
  medium_batch_weight: 0.10           # Neutral weight for medium batches (2-3)
  high_confidence_lora_weight: 0.25   # LoRA advantage for high confidence (≥0.99)
  low_confidence_traditional_weight: 0.25 # Traditional for lower confidence (≤0.9)
  low_latency_lora_weight: 0.30       # LoRA advantage for low latency (≤2000ms)
  high_latency_traditional_weight: 0.10 # Traditional acceptable for relaxed timing
  performance_history_weight: 0.20    # Historical performance comparison factor
  # Traditional model specific configurations
  traditional_bert_confidence_threshold: 0.95      # Traditional BERT confidence threshold
  traditional_modernbert_confidence_threshold: 0.8 # Traditional ModernBERT confidence threshold
  traditional_pii_detection_threshold: 0.5         # Traditional PII detection confidence threshold
  traditional_token_classification_threshold: 0.9  # Traditional token classification threshold
  traditional_dropout_prob: 0.1                    # Traditional model dropout probability
  traditional_attention_dropout_prob: 0.1          # Traditional model attention dropout probability
  tie_break_confidence: 0.5                        # Confidence value for tie-breaking situations

default_model: openai/gpt-oss-20b

# Reasoning family configurations
reasoning_families:
  deepseek:
    type: "chat_template_kwargs"
    parameter: "thinking"

  qwen3:
    type: "chat_template_kwargs"
    parameter: "enable_thinking"

  gpt-oss:
    type: "reasoning_effort"
    parameter: "reasoning_effort"
  gpt:
    type: "reasoning_effort"
    parameter: "reasoning_effort"

# Global default reasoning effort level
default_reasoning_effort: high

# API Configuration
api:
  batch_classification:
    max_batch_size: 100
    concurrency_threshold: 5
    max_concurrency: 8
    metrics:
      enabled: true
      detailed_goroutine_tracking: true
      high_resolution_timing: false
      sample_rate: 1.0
      duration_buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10, 30]
      size_buckets: [1, 2, 5, 10, 20, 50, 100, 200]

# Embedding Models Configuration
# These models provide intelligent embedding generation with automatic routing:
# - Qwen3-Embedding-0.6B: Up to 32K context, high quality, 
# - EmbeddingGemma-300M: Up to 8K context, fast inference, Matryoshka support (768/512/256/128)
embedding_models:
  qwen3_model_path: "models/Qwen3-Embedding-0.6B"
  gemma_model_path: "models/embeddinggemma-300m"
  use_cpu: true  # Set to false for GPU acceleration (requires CUDA)
